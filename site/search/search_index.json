{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the SexyCoders Docs Our sexy homepage sexycoders.org . For further assistance please contact us at team@sexycoders.org .","title":"Home"},{"location":"#welcome-to-the-sexycoders-docs","text":"Our sexy homepage sexycoders.org . For further assistance please contact us at team@sexycoders.org .","title":"Welcome to the SexyCoders Docs"},{"location":"api/authentication/","text":"Token retrieval and Parsing Getting a Token Request a token from the authentication server. The following CURL command can be used, but any equivalent in another language will work as well: curl -X POST \"https://sso.sexycoders.org/auth/realms/sexycoders.org/protocol/openid-connect/token\" \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"client_id=your_client_id\" \\ -d \"client_secret=your_client_secret\" \\ -d \"grant_type=client_credentials\" This is a client grant; passwords or usernames are not required. However, the client secret must be kept absolutely safe. Never share it and always use HTTPS when posting. The server should refuse non-HTTPS requests, but it is essential to ensure this. Dont forget to replace placeholders with your actual credentials! Expected Response The server will respond with something like the following: { \"access_token\": \"...\", \"expires_in\": 300, \"refresh_expires_in\": 0, \"token_type\": \"Bearer\", \"not-before-policy\": 0, \"scope\": \"email profile\" } The 'access_token' field is the token you need. Posting to the API With the token, you can post to the API using the following command: curl -X POST -H \"Content-Type: application/json\" \\ -d '{\"token\": \"your_token_here\"}' \\ https://your_api/path_to_endpoint Replace \"your_api/path_to_endpoint\" with the actual api endpoint location and \"your_token_here\" with the token you obtained use the correct endpoint path.","title":"Authentication"},{"location":"api/authentication/#token-retrieval-and-parsing","text":"","title":"Token retrieval and Parsing"},{"location":"api/authentication/#getting-a-token","text":"Request a token from the authentication server. The following CURL command can be used, but any equivalent in another language will work as well: curl -X POST \"https://sso.sexycoders.org/auth/realms/sexycoders.org/protocol/openid-connect/token\" \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"client_id=your_client_id\" \\ -d \"client_secret=your_client_secret\" \\ -d \"grant_type=client_credentials\" This is a client grant; passwords or usernames are not required. However, the client secret must be kept absolutely safe. Never share it and always use HTTPS when posting. The server should refuse non-HTTPS requests, but it is essential to ensure this. Dont forget to replace placeholders with your actual credentials!","title":"Getting a Token"},{"location":"api/authentication/#expected-response","text":"The server will respond with something like the following: { \"access_token\": \"...\", \"expires_in\": 300, \"refresh_expires_in\": 0, \"token_type\": \"Bearer\", \"not-before-policy\": 0, \"scope\": \"email profile\" } The 'access_token' field is the token you need.","title":"Expected Response"},{"location":"api/authentication/#posting-to-the-api","text":"With the token, you can post to the API using the following command: curl -X POST -H \"Content-Type: application/json\" \\ -d '{\"token\": \"your_token_here\"}' \\ https://your_api/path_to_endpoint Replace \"your_api/path_to_endpoint\" with the actual api endpoint location and \"your_token_here\" with the token you obtained use the correct endpoint path.","title":"Posting to the API"},{"location":"api/verify/","text":"Authentication and API Usage Token Verification curl -X POST \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"client_id=<your_client_id>\" \\ -d \"client_secret=<your_client_secret>\"\\ -d \"token=<token_value>\" \\ https://sso.sexycoders.org/auth/realms/<your_realm>/protocol/openid-connect/token/introspect This will verify if the token is active or not by returning: HUGE PILE OF JSON SHIT or {\"active\":false} You can then procceed to the execution or not of the rest of the code. Formatted response As you might notice by running the above,it returns a lot of pretty much useless \ud83d\udca9. We can get a formatted and usable return by posting the userinfo endpoint https://sso.sexycoders.org/auth/realms/<your_realm>/protocol/openid-connect/userinfo The response for an active token will then look like this: { \"sub\":\"...\", \"email_verified\":true, \"preferred_username\":\"username\", \"email\":\"email\" } A lot better aint it ? This will also include fields like Name or Last Name if present in the users profile. Endpoint listing We can find the list of available endpoints by using https://sso.sexycoders.org/auth/realms/<your_realm>/.well-known/openid-configuration Response should be a json file with the openid config of the server. It will also contain the available endpoints.","title":"Token Verification"},{"location":"api/verify/#authentication-and-api-usage","text":"","title":"Authentication and API Usage"},{"location":"api/verify/#token-verification","text":"curl -X POST \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"client_id=<your_client_id>\" \\ -d \"client_secret=<your_client_secret>\"\\ -d \"token=<token_value>\" \\ https://sso.sexycoders.org/auth/realms/<your_realm>/protocol/openid-connect/token/introspect This will verify if the token is active or not by returning: HUGE PILE OF JSON SHIT or {\"active\":false} You can then procceed to the execution or not of the rest of the code.","title":"Token Verification"},{"location":"api/verify/#formatted-response","text":"As you might notice by running the above,it returns a lot of pretty much useless \ud83d\udca9. We can get a formatted and usable return by posting the userinfo endpoint https://sso.sexycoders.org/auth/realms/<your_realm>/protocol/openid-connect/userinfo The response for an active token will then look like this: { \"sub\":\"...\", \"email_verified\":true, \"preferred_username\":\"username\", \"email\":\"email\" } A lot better aint it ? This will also include fields like Name or Last Name if present in the users profile.","title":"Formatted response"},{"location":"api/verify/#endpoint-listing","text":"We can find the list of available endpoints by using https://sso.sexycoders.org/auth/realms/<your_realm>/.well-known/openid-configuration Response should be a json file with the openid config of the server. It will also contain the available endpoints.","title":"Endpoint listing"},{"location":"cheatsheets/docker_api/","text":"Docker Swarm API Overview with Examples This is just a cheatsheet aiming to serve as a quick refference for finding what you need. If you need more info the Docker API documentation can be found at: Docker API documentation This link takes you to the Docker Engine API documentation. From there, you can select the version of the API you're interested in and browse the various endpoints in detail. Always ensure you're looking at the correct version of the documentation to match your Docker Engine version. Production Swarm The examples bellow assume you are running a test swarm and have access to the local socket of docker. In production environments this will not be the case. If you are looking to setup a production ready setup, please also read the \"Docker API in Container\" guide. 1. Containers : List all containers : Endpoint: GET /containers/json bash curl -s --unix-socket /var/run/docker.sock http://localhost/containers/json | jq Create a container : Endpoint: POST /containers/create bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"Image\":\"alpine\", \"Cmd\":[\"echo\", \"hello world\"]}' http://localhost/containers/create | jq Inspect a container : Endpoint: GET /containers/{id}/json bash curl -s --unix-socket /var/run/docker.sock http://localhost/containers/{id}/json | jq List processes running inside a container : Endpoint: GET /containers/{id}/top bash curl -s --unix-socket /var/run/docker.sock http://localhost/containers/{id}/top | jq Get logs from a container : Endpoint: GET /containers/{id}/logs bash curl -s --unix-socket /var/run/docker.sock http://localhost/containers/{id}/logs?stdout=true | jq Start a container : Endpoint: POST /containers/{id}/start bash curl -s --unix-socket /var/run/docker.sock -X POST http://localhost/containers/{id}/start Stop a container : Endpoint: POST /containers/{id}/stop bash curl -s --unix-socket /var/run/docker.sock -X POST http://localhost/containers/{id}/stop Restart a container : Endpoint: POST /containers/{id}/restart bash curl -s --unix-socket /var/run/docker.sock -X POST http://localhost/containers/{id}/restart Kill a container : Endpoint: POST /containers/{id}/kill bash curl -s --unix-socket /var/run/docker.sock -X POST http://localhost/containers/{id}/kill Delete a container : Endpoint: DELETE /containers/{id} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/containers/{id} 2. Images : List images : Endpoint: GET /images/json bash curl -s --unix-socket /var/run/docker.sock http://localhost/images/json | jq Inspect an image : Endpoint: GET /images/{name}/json bash curl -s --unix-socket /var/run/docker.sock http://localhost/images/alpine/json | jq Pull an image : Endpoint: POST /images/create bash curl -s --unix-socket /var/run/docker.sock -X POST -d 'fromImage=alpine' http://localhost/images/create | jq Remove an image : Endpoint: DELETE /images/{name} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/images/alpine 3. Nodes (Swarm Specific) : List nodes : Endpoint: GET /nodes bash curl -s --unix-socket /var/run/docker.sock http://localhost/nodes | jq Inspect a node : Endpoint: GET /nodes/{id} bash curl -s --unix-socket /var/run/docker.sock http://localhost/nodes/{id} | jq Update a node : Endpoint: POST /nodes/{id}/update bash # Must provide version and other details in the request body Delete a node : Endpoint: DELETE /nodes/{id} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/nodes/{id} 4. Services (Swarm Specific) : List services : Endpoint: GET /services bash curl -s --unix-socket /var/run/docker.sock http://localhost/services | jq Create a service : Endpoint: POST /services/create bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"Name\":\"myservice\", \"TaskTemplate\": {\"ContainerSpec\": {\"Image\":\"alpine\", \"Command\":[\"sleep\", \"3600\"]}}}' http://localhost/services/create | jq Inspect a service : Endpoint: GET /services/{id} bash curl -s --unix-socket /var/run/docker.sock http://localhost/services/{id} | jq Update a service : Endpoint: POST /services/{id}/update bash # This request needs additional details like the version of the service and the new configuration. Delete a service : Endpoint: DELETE /services/{id} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/services/{id} 5. Tasks (Swarm Specific) : List tasks : Endpoint: GET /tasks bash curl -s --unix-socket /var/run/docker.sock http://localhost/tasks | jq Inspect a task : Endpoint: GET /tasks/{id} bash curl -s --unix-socket /var/run/docker.sock http://localhost/tasks/{id} | jq 6. Secrets (Swarm Specific) : List secrets : Endpoint: GET /secrets bash curl -s --unix-socket /var/run/docker.sock http://localhost/secrets | jq Create a secret : Endpoint: POST /secrets/create bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"Name\":\"mysecret\", \"Data\":\"base64_encoded_data\"}' http://localhost/secrets/create | jq Inspect a secret : Endpoint: GET /secrets/{id} bash curl -s --unix-socket /var/run/docker.sock http://localhost/secrets/{id} | jq Delete a secret : Endpoint: DELETE /secrets/{id} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/secrets/{id} 7. Configs (Swarm Specific) : List configs : Endpoint: GET /configs bash curl -s --unix-socket /var/run/docker.sock http://localhost/configs | jq Create a config : Endpoint: POST /configs/create bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"Name\":\"myconfig\", \"Data\":\"base64_encoded_data\"}' http://localhost/configs/create | jq Inspect a config : Endpoint: GET /configs/{id} bash curl -s --unix-socket /var/run/docker.sock http://localhost/configs/{id} | jq Delete a config : Endpoint: DELETE /configs/{id} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/configs/{id} 8. Plugins : List plugins : Endpoint: GET /plugins bash curl -s --unix-socket /var/run/docker.sock http://localhost/plugins | jq Inspect a plugin : Endpoint: GET /plugins/{name}/json bash curl -s --unix-socket /var/run/docker.sock http://localhost/plugins/{name}/json | jq Install a plugin : Endpoint: POST /plugins/pull bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"Name\":\"plugin_name\"}' http://localhost/plugins/pull Enable a plugin : Endpoint: POST /plugins/{name}/enable bash curl -s --unix-socket /var/run/docker.sock -X POST http://localhost/plugins/{name}/enable Disable a plugin : Endpoint: POST /plugins/{name}/disable bash curl -s --unix-socket /var/run/docker.sock -X POST http://localhost/plugins/{name}/disable Remove a plugin : Endpoint: DELETE /plugins/{name} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/plugins/{name} 9. Networks : List networks : Endpoint: GET /networks bash curl -s --unix-socket /var/run/docker.sock http://localhost/networks | jq Inspect a network : Endpoint: GET /networks/{id} bash curl -s --unix-socket /var/run/docker.sock http://localhost/networks/{id} | jq Create a network : Endpoint: POST /networks/create bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"Name\":\"mynetwork\", \"Driver\":\"overlay\"}' http://localhost/networks/create | jq Remove a network : Endpoint: DELETE /networks/{id} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/networks/{id} 10. Volumes : List volumes : Endpoint: GET /volumes bash curl -s --unix-socket /var/run/docker.sock http://localhost/volumes | jq Inspect a volume : Endpoint: GET /volumes/{name} bash curl -s --unix-socket /var/run/docker.sock http://localhost/volumes/{name} | jq Create a volume : Endpoint: POST /volumes/create bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"Name\":\"myvolume\"}' http://localhost/volumes/create | jq Remove a volume : Endpoint: DELETE /volumes/{name} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/volumes/{name} 11. System : Get Docker version : Endpoint: GET /version bash curl -s --unix-socket /var/run/docker.sock http://localhost/version | jq Get system information : Endpoint: GET /info bash curl -s --unix-socket /var/run/docker.sock http://localhost/info | jq 12. Exec : Create exec instance : Endpoint: POST /containers/{id}/exec bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"AttachStdin\":false, \"AttachStdout\":true, \"AttachStderr\":true, \"Cmd\":[\"date\"]}' http://localhost/containers/{id}/exec | jq Start exec instance : Endpoint: POST /exec/{id}/start bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"Detach\": false, \"Tty\": false}' http://localhost/exec/{id}/start | jq","title":"Docker api"},{"location":"cheatsheets/docker_api/#docker-swarm-api-overview-with-examples","text":"This is just a cheatsheet aiming to serve as a quick refference for finding what you need. If you need more info the Docker API documentation can be found at: Docker API documentation This link takes you to the Docker Engine API documentation. From there, you can select the version of the API you're interested in and browse the various endpoints in detail. Always ensure you're looking at the correct version of the documentation to match your Docker Engine version.","title":"Docker Swarm API Overview with Examples"},{"location":"cheatsheets/docker_api/#production-swarm","text":"The examples bellow assume you are running a test swarm and have access to the local socket of docker. In production environments this will not be the case. If you are looking to setup a production ready setup, please also read the \"Docker API in Container\" guide.","title":"Production Swarm"},{"location":"cheatsheets/docker_api/#1-containers","text":"List all containers : Endpoint: GET /containers/json bash curl -s --unix-socket /var/run/docker.sock http://localhost/containers/json | jq Create a container : Endpoint: POST /containers/create bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"Image\":\"alpine\", \"Cmd\":[\"echo\", \"hello world\"]}' http://localhost/containers/create | jq Inspect a container : Endpoint: GET /containers/{id}/json bash curl -s --unix-socket /var/run/docker.sock http://localhost/containers/{id}/json | jq List processes running inside a container : Endpoint: GET /containers/{id}/top bash curl -s --unix-socket /var/run/docker.sock http://localhost/containers/{id}/top | jq Get logs from a container : Endpoint: GET /containers/{id}/logs bash curl -s --unix-socket /var/run/docker.sock http://localhost/containers/{id}/logs?stdout=true | jq Start a container : Endpoint: POST /containers/{id}/start bash curl -s --unix-socket /var/run/docker.sock -X POST http://localhost/containers/{id}/start Stop a container : Endpoint: POST /containers/{id}/stop bash curl -s --unix-socket /var/run/docker.sock -X POST http://localhost/containers/{id}/stop Restart a container : Endpoint: POST /containers/{id}/restart bash curl -s --unix-socket /var/run/docker.sock -X POST http://localhost/containers/{id}/restart Kill a container : Endpoint: POST /containers/{id}/kill bash curl -s --unix-socket /var/run/docker.sock -X POST http://localhost/containers/{id}/kill Delete a container : Endpoint: DELETE /containers/{id} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/containers/{id}","title":"1. Containers:"},{"location":"cheatsheets/docker_api/#2-images","text":"List images : Endpoint: GET /images/json bash curl -s --unix-socket /var/run/docker.sock http://localhost/images/json | jq Inspect an image : Endpoint: GET /images/{name}/json bash curl -s --unix-socket /var/run/docker.sock http://localhost/images/alpine/json | jq Pull an image : Endpoint: POST /images/create bash curl -s --unix-socket /var/run/docker.sock -X POST -d 'fromImage=alpine' http://localhost/images/create | jq Remove an image : Endpoint: DELETE /images/{name} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/images/alpine","title":"2. Images:"},{"location":"cheatsheets/docker_api/#3-nodes-swarm-specific","text":"List nodes : Endpoint: GET /nodes bash curl -s --unix-socket /var/run/docker.sock http://localhost/nodes | jq Inspect a node : Endpoint: GET /nodes/{id} bash curl -s --unix-socket /var/run/docker.sock http://localhost/nodes/{id} | jq Update a node : Endpoint: POST /nodes/{id}/update bash # Must provide version and other details in the request body Delete a node : Endpoint: DELETE /nodes/{id} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/nodes/{id}","title":"3. Nodes (Swarm Specific):"},{"location":"cheatsheets/docker_api/#4-services-swarm-specific","text":"List services : Endpoint: GET /services bash curl -s --unix-socket /var/run/docker.sock http://localhost/services | jq Create a service : Endpoint: POST /services/create bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"Name\":\"myservice\", \"TaskTemplate\": {\"ContainerSpec\": {\"Image\":\"alpine\", \"Command\":[\"sleep\", \"3600\"]}}}' http://localhost/services/create | jq Inspect a service : Endpoint: GET /services/{id} bash curl -s --unix-socket /var/run/docker.sock http://localhost/services/{id} | jq Update a service : Endpoint: POST /services/{id}/update bash # This request needs additional details like the version of the service and the new configuration. Delete a service : Endpoint: DELETE /services/{id} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/services/{id}","title":"4. Services (Swarm Specific):"},{"location":"cheatsheets/docker_api/#5-tasks-swarm-specific","text":"List tasks : Endpoint: GET /tasks bash curl -s --unix-socket /var/run/docker.sock http://localhost/tasks | jq Inspect a task : Endpoint: GET /tasks/{id} bash curl -s --unix-socket /var/run/docker.sock http://localhost/tasks/{id} | jq","title":"5. Tasks (Swarm Specific):"},{"location":"cheatsheets/docker_api/#6-secrets-swarm-specific","text":"List secrets : Endpoint: GET /secrets bash curl -s --unix-socket /var/run/docker.sock http://localhost/secrets | jq Create a secret : Endpoint: POST /secrets/create bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"Name\":\"mysecret\", \"Data\":\"base64_encoded_data\"}' http://localhost/secrets/create | jq Inspect a secret : Endpoint: GET /secrets/{id} bash curl -s --unix-socket /var/run/docker.sock http://localhost/secrets/{id} | jq Delete a secret : Endpoint: DELETE /secrets/{id} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/secrets/{id}","title":"6. Secrets (Swarm Specific):"},{"location":"cheatsheets/docker_api/#7-configs-swarm-specific","text":"List configs : Endpoint: GET /configs bash curl -s --unix-socket /var/run/docker.sock http://localhost/configs | jq Create a config : Endpoint: POST /configs/create bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"Name\":\"myconfig\", \"Data\":\"base64_encoded_data\"}' http://localhost/configs/create | jq Inspect a config : Endpoint: GET /configs/{id} bash curl -s --unix-socket /var/run/docker.sock http://localhost/configs/{id} | jq Delete a config : Endpoint: DELETE /configs/{id} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/configs/{id}","title":"7. Configs (Swarm Specific):"},{"location":"cheatsheets/docker_api/#8-plugins","text":"List plugins : Endpoint: GET /plugins bash curl -s --unix-socket /var/run/docker.sock http://localhost/plugins | jq Inspect a plugin : Endpoint: GET /plugins/{name}/json bash curl -s --unix-socket /var/run/docker.sock http://localhost/plugins/{name}/json | jq Install a plugin : Endpoint: POST /plugins/pull bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"Name\":\"plugin_name\"}' http://localhost/plugins/pull Enable a plugin : Endpoint: POST /plugins/{name}/enable bash curl -s --unix-socket /var/run/docker.sock -X POST http://localhost/plugins/{name}/enable Disable a plugin : Endpoint: POST /plugins/{name}/disable bash curl -s --unix-socket /var/run/docker.sock -X POST http://localhost/plugins/{name}/disable Remove a plugin : Endpoint: DELETE /plugins/{name} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/plugins/{name}","title":"8. Plugins:"},{"location":"cheatsheets/docker_api/#9-networks","text":"List networks : Endpoint: GET /networks bash curl -s --unix-socket /var/run/docker.sock http://localhost/networks | jq Inspect a network : Endpoint: GET /networks/{id} bash curl -s --unix-socket /var/run/docker.sock http://localhost/networks/{id} | jq Create a network : Endpoint: POST /networks/create bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"Name\":\"mynetwork\", \"Driver\":\"overlay\"}' http://localhost/networks/create | jq Remove a network : Endpoint: DELETE /networks/{id} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/networks/{id}","title":"9. Networks:"},{"location":"cheatsheets/docker_api/#10-volumes","text":"List volumes : Endpoint: GET /volumes bash curl -s --unix-socket /var/run/docker.sock http://localhost/volumes | jq Inspect a volume : Endpoint: GET /volumes/{name} bash curl -s --unix-socket /var/run/docker.sock http://localhost/volumes/{name} | jq Create a volume : Endpoint: POST /volumes/create bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"Name\":\"myvolume\"}' http://localhost/volumes/create | jq Remove a volume : Endpoint: DELETE /volumes/{name} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/volumes/{name}","title":"10. Volumes:"},{"location":"cheatsheets/docker_api/#11-system","text":"Get Docker version : Endpoint: GET /version bash curl -s --unix-socket /var/run/docker.sock http://localhost/version | jq Get system information : Endpoint: GET /info bash curl -s --unix-socket /var/run/docker.sock http://localhost/info | jq","title":"11. System:"},{"location":"cheatsheets/docker_api/#12-exec","text":"Create exec instance : Endpoint: POST /containers/{id}/exec bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"AttachStdin\":false, \"AttachStdout\":true, \"AttachStderr\":true, \"Cmd\":[\"date\"]}' http://localhost/containers/{id}/exec | jq Start exec instance : Endpoint: POST /exec/{id}/start bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"Detach\": false, \"Tty\": false}' http://localhost/exec/{id}/start | jq","title":"12. Exec:"},{"location":"cookbook/docker_api_in_container/","text":"Exposing Docker Daemon in Docker Swarm Nodes This document presents a solution to expose the Docker daemon on each node of a Docker Swarm cluster, allowing a management container to always interact with the Docker daemon of its current host node. Overview In a Docker Swarm setup, containers (or tasks) can be rescheduled or moved between nodes due to various reasons like node failures, scaling actions, or updates. For containers that need to interact with the Docker API, we aim to ensure they can always communicate with the Docker daemon of their host node. Solution 1. Global Docker API Proxy Service Goal Deploy a global service that exposes the Docker daemon of each node. Implementation Instead of using docker:dind (Docker in Docker), a lightweight proxy tool like socat will forward traffic from a container to the host's Docker socket. Docker Compose syntax for the service: services: docker-api-proxy: image: alpine/socat command: tcp-listen:2375,fork,reuseaddr unix-connect:/var/run/docker.sock deploy: mode: global 2. Management Container Goal Ensure the management container (e.g., a Node app) can communicate with the Docker daemon of its host node. Implementation No matter which node the management container is scheduled on, it can communicate with its local Docker daemon by addressing localhost:2375 . Important Considerations Swarm Mode Limitations Swarm mode has a set of API endpoints distinct from a regular Docker daemon. If the management app plans to manipulate Swarm-level features, it must handle these API differences. Security Exposing the Docker API can lead to security vulnerabilities: Even if exposed over localhost , there's a risk if a container manages to break its sandbox or if a malicious process runs on the node. Using TLS and client certificates can limit unauthorized external access, but local threats remain a concern. Direct Exposure vs. Nested Docker Daemon Using docker:dind means running a Docker daemon inside a Docker container, which has potential issues like data loss and performance concerns. Our chosen method directly connects the exposed TCP port in the container to the Docker socket on the host. Conclusion This setup ensures that a management container in a Docker Swarm environment can reliably and consistently communicate with the Docker daemon of its current host node. As always, careful attention to security and potential limitations is crucial.","title":"Exposing Docker Daemon in Docker Swarm Nodes"},{"location":"cookbook/docker_api_in_container/#exposing-docker-daemon-in-docker-swarm-nodes","text":"This document presents a solution to expose the Docker daemon on each node of a Docker Swarm cluster, allowing a management container to always interact with the Docker daemon of its current host node.","title":"Exposing Docker Daemon in Docker Swarm Nodes"},{"location":"cookbook/docker_api_in_container/#overview","text":"In a Docker Swarm setup, containers (or tasks) can be rescheduled or moved between nodes due to various reasons like node failures, scaling actions, or updates. For containers that need to interact with the Docker API, we aim to ensure they can always communicate with the Docker daemon of their host node.","title":"Overview"},{"location":"cookbook/docker_api_in_container/#solution","text":"","title":"Solution"},{"location":"cookbook/docker_api_in_container/#1-global-docker-api-proxy-service","text":"","title":"1. Global Docker API Proxy Service"},{"location":"cookbook/docker_api_in_container/#goal","text":"Deploy a global service that exposes the Docker daemon of each node.","title":"Goal"},{"location":"cookbook/docker_api_in_container/#implementation","text":"Instead of using docker:dind (Docker in Docker), a lightweight proxy tool like socat will forward traffic from a container to the host's Docker socket. Docker Compose syntax for the service: services: docker-api-proxy: image: alpine/socat command: tcp-listen:2375,fork,reuseaddr unix-connect:/var/run/docker.sock deploy: mode: global","title":"Implementation"},{"location":"cookbook/docker_api_in_container/#2-management-container","text":"","title":"2. Management Container"},{"location":"cookbook/docker_api_in_container/#goal_1","text":"Ensure the management container (e.g., a Node app) can communicate with the Docker daemon of its host node.","title":"Goal"},{"location":"cookbook/docker_api_in_container/#implementation_1","text":"No matter which node the management container is scheduled on, it can communicate with its local Docker daemon by addressing localhost:2375 .","title":"Implementation"},{"location":"cookbook/docker_api_in_container/#important-considerations","text":"","title":"Important Considerations"},{"location":"cookbook/docker_api_in_container/#swarm-mode-limitations","text":"Swarm mode has a set of API endpoints distinct from a regular Docker daemon. If the management app plans to manipulate Swarm-level features, it must handle these API differences.","title":"Swarm Mode Limitations"},{"location":"cookbook/docker_api_in_container/#security","text":"Exposing the Docker API can lead to security vulnerabilities: Even if exposed over localhost , there's a risk if a container manages to break its sandbox or if a malicious process runs on the node. Using TLS and client certificates can limit unauthorized external access, but local threats remain a concern.","title":"Security"},{"location":"cookbook/docker_api_in_container/#direct-exposure-vs-nested-docker-daemon","text":"Using docker:dind means running a Docker daemon inside a Docker container, which has potential issues like data loss and performance concerns. Our chosen method directly connects the exposed TCP port in the container to the Docker socket on the host.","title":"Direct Exposure vs. Nested Docker Daemon"},{"location":"cookbook/docker_api_in_container/#conclusion","text":"This setup ensures that a management container in a Docker Swarm environment can reliably and consistently communicate with the Docker daemon of its current host node. As always, careful attention to security and potential limitations is crucial.","title":"Conclusion"},{"location":"copy/gpg/","text":"Generating a Key with GPG This guide provides step-by-step instructions on generating both Ed25519 and RSA keys using GPG. Table of Contents Prerequisites Generating an Ed25519 Key Generating an RSA Key Managing GPG Keys Prerequisites GnuPG installed. Generating an Ed25519 Key Open a Terminal or Command Prompt . Initiate Key Generation : bash gpg --expert --full-generate-key Select Key Type : Choose (E) ECC and ECC when prompted. Choose the Curve : Select Ed25519 . Set Key Expiry : Recommended: 2y (or less) for a balance between security and convenience. Enter User Details : Real name: Your name. Email address: Your email. Comment: Optional. Set a Strong Passphrase to protect your private key. Verify Key Generation : bash gpg --list-keys your_email@example.com Generating an RSA Key Open a Terminal or Command Prompt . Initiate Key Generation : bash gpg --full-generate-key Select Key Type : Choose (1) RSA and RSA . Key Size : Choose 4096 bits for enhanced security. Set Key Expiry : Recommended: 2y (or less) for a balance between security and convenience. Enter User Details : Real name: Your name. Email address: Your email. Comment: Optional. Set a Strong Passphrase to protect your private key. Verify Key Generation : bash gpg --list-keys your_email@example.com Managing GPG Keys Deleting a Key List All Keys : bash gpg --list-keys Delete the Desired Key : bash gpg --delete-keys KEY_ID Confirm Deletion when prompted. Delete the Associated Private Key (if needed) : bash gpg --delete-secret-keys KEY_ID Configuring Thunderbird with GPG This guide provides step-by-step instructions on integrating Thunderbird with GPG for secure email communication. Table of Contents Prerequisites Configuring Thunderbird Prerequisites Thunderbird installed. GnuPG installed. Configuring Thunderbird Open Thunderbird . Access Configuration Editor : Navigate to: Thunderbird \u2192 Settings \u2192 General \u2192 Config Editor. Enable External GnuPG : Search for mail.openpgp.allow_external_gnupg . Set its value to true . Restart Thunderbird for the changes to take effect.","title":"GPG Keys"},{"location":"copy/gpg/#generating-a-key-with-gpg","text":"This guide provides step-by-step instructions on generating both Ed25519 and RSA keys using GPG.","title":"Generating a Key with GPG"},{"location":"copy/gpg/#table-of-contents","text":"Prerequisites Generating an Ed25519 Key Generating an RSA Key Managing GPG Keys","title":"Table of Contents"},{"location":"copy/gpg/#prerequisites","text":"GnuPG installed.","title":"Prerequisites"},{"location":"copy/gpg/#generating-an-ed25519-key","text":"Open a Terminal or Command Prompt . Initiate Key Generation : bash gpg --expert --full-generate-key Select Key Type : Choose (E) ECC and ECC when prompted. Choose the Curve : Select Ed25519 . Set Key Expiry : Recommended: 2y (or less) for a balance between security and convenience. Enter User Details : Real name: Your name. Email address: Your email. Comment: Optional. Set a Strong Passphrase to protect your private key. Verify Key Generation : bash gpg --list-keys your_email@example.com","title":"Generating an Ed25519 Key"},{"location":"copy/gpg/#generating-an-rsa-key","text":"Open a Terminal or Command Prompt . Initiate Key Generation : bash gpg --full-generate-key Select Key Type : Choose (1) RSA and RSA . Key Size : Choose 4096 bits for enhanced security. Set Key Expiry : Recommended: 2y (or less) for a balance between security and convenience. Enter User Details : Real name: Your name. Email address: Your email. Comment: Optional. Set a Strong Passphrase to protect your private key. Verify Key Generation : bash gpg --list-keys your_email@example.com","title":"Generating an RSA Key"},{"location":"copy/gpg/#managing-gpg-keys","text":"","title":"Managing GPG Keys"},{"location":"copy/gpg/#deleting-a-key","text":"List All Keys : bash gpg --list-keys Delete the Desired Key : bash gpg --delete-keys KEY_ID Confirm Deletion when prompted. Delete the Associated Private Key (if needed) : bash gpg --delete-secret-keys KEY_ID","title":"Deleting a Key"},{"location":"copy/gpg/#configuring-thunderbird-with-gpg","text":"This guide provides step-by-step instructions on integrating Thunderbird with GPG for secure email communication.","title":"Configuring Thunderbird with GPG"},{"location":"copy/gpg/#table-of-contents_1","text":"Prerequisites Configuring Thunderbird","title":"Table of Contents"},{"location":"copy/gpg/#prerequisites_1","text":"Thunderbird installed. GnuPG installed.","title":"Prerequisites"},{"location":"copy/gpg/#configuring-thunderbird","text":"Open Thunderbird . Access Configuration Editor : Navigate to: Thunderbird \u2192 Settings \u2192 General \u2192 Config Editor. Enable External GnuPG : Search for mail.openpgp.allow_external_gnupg . Set its value to true . Restart Thunderbird for the changes to take effect.","title":"Configuring Thunderbird"},{"location":"copy/thunderbird_gpg/","text":"Configuring Thunderbird with GPG This guide provides step-by-step instructions on integrating Thunderbird with GPG for secure email communication. Table of Contents Prerequisites Configuring Thunderbird Additional Information References Prerequisites Thunderbird installed. GnuPG installed. Configuring Thunderbird Open Thunderbird . Access Configuration Editor : Navigate to: Thunderbird \u2192 Settings \u2192 General \u2192 Config Editor. Enable External GnuPG : Search for mail.openpgp.allow_external_gnupg . Set its value to true . Restart Thunderbird for the changes to take effect. Additional Information Direct Integration Limitation : Thunderbird does not directly utilize the existing GPG keyring for public keys. This is a design choice by Thunderbird. Users might have to maintain two databases with the same content, storing contacts' trusted keys and their level of trust. Using GPG-Agent with Thunderbird : While direct integration is not possible, Thunderbird can be configured to use GnuPGP's gpg-agent . This allows Thunderbird to use the keys of GnuPGP. For more details, refer to the Mozilla wiki page . Note that the page primarily discusses smartcards, but the linked section is generic about using gpg-agent with Thunderbird. References SuperUser Discussion on Thunderbird and GPG Keyring","title":"Thunderbird GPG Keys"},{"location":"copy/thunderbird_gpg/#configuring-thunderbird-with-gpg","text":"This guide provides step-by-step instructions on integrating Thunderbird with GPG for secure email communication.","title":"Configuring Thunderbird with GPG"},{"location":"copy/thunderbird_gpg/#table-of-contents","text":"Prerequisites Configuring Thunderbird Additional Information References","title":"Table of Contents"},{"location":"copy/thunderbird_gpg/#prerequisites","text":"Thunderbird installed. GnuPG installed.","title":"Prerequisites"},{"location":"copy/thunderbird_gpg/#configuring-thunderbird","text":"Open Thunderbird . Access Configuration Editor : Navigate to: Thunderbird \u2192 Settings \u2192 General \u2192 Config Editor. Enable External GnuPG : Search for mail.openpgp.allow_external_gnupg . Set its value to true . Restart Thunderbird for the changes to take effect.","title":"Configuring Thunderbird"},{"location":"copy/thunderbird_gpg/#additional-information","text":"Direct Integration Limitation : Thunderbird does not directly utilize the existing GPG keyring for public keys. This is a design choice by Thunderbird. Users might have to maintain two databases with the same content, storing contacts' trusted keys and their level of trust. Using GPG-Agent with Thunderbird : While direct integration is not possible, Thunderbird can be configured to use GnuPGP's gpg-agent . This allows Thunderbird to use the keys of GnuPGP. For more details, refer to the Mozilla wiki page . Note that the page primarily discusses smartcards, but the linked section is generic about using gpg-agent with Thunderbird.","title":"Additional Information"},{"location":"copy/thunderbird_gpg/#references","text":"SuperUser Discussion on Thunderbird and GPG Keyring","title":"References"},{"location":"copy/docker/docker_api/","text":"Docker Swarm API Overview with Examples This is just a cheatsheet aiming to serve as a quick refference for finding what you need. If you need more info the Docker API documentation can be found at: Docker API documentation This link takes you to the Docker Engine API documentation. From there, you can select the version of the API you're interested in and browse the various endpoints in detail. Always ensure you're looking at the correct version of the documentation to match your Docker Engine version. Production Swarm The examples bellow assume you are running a test swarm and have access to the local socket of docker. In production environments this will not be the case. If you are looking to setup a production ready setup, please also read the \"Docker API in Container\" guide. 1. Containers : List all containers : Endpoint: GET /containers/json bash curl -s --unix-socket /var/run/docker.sock http://localhost/containers/json | jq Create a container : Endpoint: POST /containers/create bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"Image\":\"alpine\", \"Cmd\":[\"echo\", \"hello world\"]}' http://localhost/containers/create | jq Inspect a container : Endpoint: GET /containers/{id}/json bash curl -s --unix-socket /var/run/docker.sock http://localhost/containers/{id}/json | jq List processes running inside a container : Endpoint: GET /containers/{id}/top bash curl -s --unix-socket /var/run/docker.sock http://localhost/containers/{id}/top | jq Get logs from a container : Endpoint: GET /containers/{id}/logs bash curl -s --unix-socket /var/run/docker.sock http://localhost/containers/{id}/logs?stdout=true | jq Start a container : Endpoint: POST /containers/{id}/start bash curl -s --unix-socket /var/run/docker.sock -X POST http://localhost/containers/{id}/start Stop a container : Endpoint: POST /containers/{id}/stop bash curl -s --unix-socket /var/run/docker.sock -X POST http://localhost/containers/{id}/stop Restart a container : Endpoint: POST /containers/{id}/restart bash curl -s --unix-socket /var/run/docker.sock -X POST http://localhost/containers/{id}/restart Kill a container : Endpoint: POST /containers/{id}/kill bash curl -s --unix-socket /var/run/docker.sock -X POST http://localhost/containers/{id}/kill Delete a container : Endpoint: DELETE /containers/{id} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/containers/{id} 2. Images : List images : Endpoint: GET /images/json bash curl -s --unix-socket /var/run/docker.sock http://localhost/images/json | jq Inspect an image : Endpoint: GET /images/{name}/json bash curl -s --unix-socket /var/run/docker.sock http://localhost/images/alpine/json | jq Pull an image : Endpoint: POST /images/create bash curl -s --unix-socket /var/run/docker.sock -X POST -d 'fromImage=alpine' http://localhost/images/create | jq Remove an image : Endpoint: DELETE /images/{name} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/images/alpine 3. Nodes (Swarm Specific) : List nodes : Endpoint: GET /nodes bash curl -s --unix-socket /var/run/docker.sock http://localhost/nodes | jq Inspect a node : Endpoint: GET /nodes/{id} bash curl -s --unix-socket /var/run/docker.sock http://localhost/nodes/{id} | jq Update a node : Endpoint: POST /nodes/{id}/update bash # Must provide version and other details in the request body Delete a node : Endpoint: DELETE /nodes/{id} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/nodes/{id} 4. Services (Swarm Specific) : List services : Endpoint: GET /services bash curl -s --unix-socket /var/run/docker.sock http://localhost/services | jq Create a service : Endpoint: POST /services/create bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"Name\":\"myservice\", \"TaskTemplate\": {\"ContainerSpec\": {\"Image\":\"alpine\", \"Command\":[\"sleep\", \"3600\"]}}}' http://localhost/services/create | jq Inspect a service : Endpoint: GET /services/{id} bash curl -s --unix-socket /var/run/docker.sock http://localhost/services/{id} | jq Update a service : Endpoint: POST /services/{id}/update bash # This request needs additional details like the version of the service and the new configuration. Delete a service : Endpoint: DELETE /services/{id} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/services/{id} 5. Tasks (Swarm Specific) : List tasks : Endpoint: GET /tasks bash curl -s --unix-socket /var/run/docker.sock http://localhost/tasks | jq Inspect a task : Endpoint: GET /tasks/{id} bash curl -s --unix-socket /var/run/docker.sock http://localhost/tasks/{id} | jq 6. Secrets (Swarm Specific) : List secrets : Endpoint: GET /secrets bash curl -s --unix-socket /var/run/docker.sock http://localhost/secrets | jq Create a secret : Endpoint: POST /secrets/create bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"Name\":\"mysecret\", \"Data\":\"base64_encoded_data\"}' http://localhost/secrets/create | jq Inspect a secret : Endpoint: GET /secrets/{id} bash curl -s --unix-socket /var/run/docker.sock http://localhost/secrets/{id} | jq Delete a secret : Endpoint: DELETE /secrets/{id} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/secrets/{id} 7. Configs (Swarm Specific) : List configs : Endpoint: GET /configs bash curl -s --unix-socket /var/run/docker.sock http://localhost/configs | jq Create a config : Endpoint: POST /configs/create bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"Name\":\"myconfig\", \"Data\":\"base64_encoded_data\"}' http://localhost/configs/create | jq Inspect a config : Endpoint: GET /configs/{id} bash curl -s --unix-socket /var/run/docker.sock http://localhost/configs/{id} | jq Delete a config : Endpoint: DELETE /configs/{id} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/configs/{id} 8. Plugins : List plugins : Endpoint: GET /plugins bash curl -s --unix-socket /var/run/docker.sock http://localhost/plugins | jq Inspect a plugin : Endpoint: GET /plugins/{name}/json bash curl -s --unix-socket /var/run/docker.sock http://localhost/plugins/{name}/json | jq Install a plugin : Endpoint: POST /plugins/pull bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"Name\":\"plugin_name\"}' http://localhost/plugins/pull Enable a plugin : Endpoint: POST /plugins/{name}/enable bash curl -s --unix-socket /var/run/docker.sock -X POST http://localhost/plugins/{name}/enable Disable a plugin : Endpoint: POST /plugins/{name}/disable bash curl -s --unix-socket /var/run/docker.sock -X POST http://localhost/plugins/{name}/disable Remove a plugin : Endpoint: DELETE /plugins/{name} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/plugins/{name} 9. Networks : List networks : Endpoint: GET /networks bash curl -s --unix-socket /var/run/docker.sock http://localhost/networks | jq Inspect a network : Endpoint: GET /networks/{id} bash curl -s --unix-socket /var/run/docker.sock http://localhost/networks/{id} | jq Create a network : Endpoint: POST /networks/create bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"Name\":\"mynetwork\", \"Driver\":\"overlay\"}' http://localhost/networks/create | jq Remove a network : Endpoint: DELETE /networks/{id} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/networks/{id} 10. Volumes : List volumes : Endpoint: GET /volumes bash curl -s --unix-socket /var/run/docker.sock http://localhost/volumes | jq Inspect a volume : Endpoint: GET /volumes/{name} bash curl -s --unix-socket /var/run/docker.sock http://localhost/volumes/{name} | jq Create a volume : Endpoint: POST /volumes/create bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"Name\":\"myvolume\"}' http://localhost/volumes/create | jq Remove a volume : Endpoint: DELETE /volumes/{name} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/volumes/{name} 11. System : Get Docker version : Endpoint: GET /version bash curl -s --unix-socket /var/run/docker.sock http://localhost/version | jq Get system information : Endpoint: GET /info bash curl -s --unix-socket /var/run/docker.sock http://localhost/info | jq 12. Exec : Create exec instance : Endpoint: POST /containers/{id}/exec bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"AttachStdin\":false, \"AttachStdout\":true, \"AttachStderr\":true, \"Cmd\":[\"date\"]}' http://localhost/containers/{id}/exec | jq Start exec instance : Endpoint: POST /exec/{id}/start bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"Detach\": false, \"Tty\": false}' http://localhost/exec/{id}/start | jq","title":"Docker API"},{"location":"copy/docker/docker_api/#docker-swarm-api-overview-with-examples","text":"This is just a cheatsheet aiming to serve as a quick refference for finding what you need. If you need more info the Docker API documentation can be found at: Docker API documentation This link takes you to the Docker Engine API documentation. From there, you can select the version of the API you're interested in and browse the various endpoints in detail. Always ensure you're looking at the correct version of the documentation to match your Docker Engine version.","title":"Docker Swarm API Overview with Examples"},{"location":"copy/docker/docker_api/#production-swarm","text":"The examples bellow assume you are running a test swarm and have access to the local socket of docker. In production environments this will not be the case. If you are looking to setup a production ready setup, please also read the \"Docker API in Container\" guide.","title":"Production Swarm"},{"location":"copy/docker/docker_api/#1-containers","text":"List all containers : Endpoint: GET /containers/json bash curl -s --unix-socket /var/run/docker.sock http://localhost/containers/json | jq Create a container : Endpoint: POST /containers/create bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"Image\":\"alpine\", \"Cmd\":[\"echo\", \"hello world\"]}' http://localhost/containers/create | jq Inspect a container : Endpoint: GET /containers/{id}/json bash curl -s --unix-socket /var/run/docker.sock http://localhost/containers/{id}/json | jq List processes running inside a container : Endpoint: GET /containers/{id}/top bash curl -s --unix-socket /var/run/docker.sock http://localhost/containers/{id}/top | jq Get logs from a container : Endpoint: GET /containers/{id}/logs bash curl -s --unix-socket /var/run/docker.sock http://localhost/containers/{id}/logs?stdout=true | jq Start a container : Endpoint: POST /containers/{id}/start bash curl -s --unix-socket /var/run/docker.sock -X POST http://localhost/containers/{id}/start Stop a container : Endpoint: POST /containers/{id}/stop bash curl -s --unix-socket /var/run/docker.sock -X POST http://localhost/containers/{id}/stop Restart a container : Endpoint: POST /containers/{id}/restart bash curl -s --unix-socket /var/run/docker.sock -X POST http://localhost/containers/{id}/restart Kill a container : Endpoint: POST /containers/{id}/kill bash curl -s --unix-socket /var/run/docker.sock -X POST http://localhost/containers/{id}/kill Delete a container : Endpoint: DELETE /containers/{id} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/containers/{id}","title":"1. Containers:"},{"location":"copy/docker/docker_api/#2-images","text":"List images : Endpoint: GET /images/json bash curl -s --unix-socket /var/run/docker.sock http://localhost/images/json | jq Inspect an image : Endpoint: GET /images/{name}/json bash curl -s --unix-socket /var/run/docker.sock http://localhost/images/alpine/json | jq Pull an image : Endpoint: POST /images/create bash curl -s --unix-socket /var/run/docker.sock -X POST -d 'fromImage=alpine' http://localhost/images/create | jq Remove an image : Endpoint: DELETE /images/{name} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/images/alpine","title":"2. Images:"},{"location":"copy/docker/docker_api/#3-nodes-swarm-specific","text":"List nodes : Endpoint: GET /nodes bash curl -s --unix-socket /var/run/docker.sock http://localhost/nodes | jq Inspect a node : Endpoint: GET /nodes/{id} bash curl -s --unix-socket /var/run/docker.sock http://localhost/nodes/{id} | jq Update a node : Endpoint: POST /nodes/{id}/update bash # Must provide version and other details in the request body Delete a node : Endpoint: DELETE /nodes/{id} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/nodes/{id}","title":"3. Nodes (Swarm Specific):"},{"location":"copy/docker/docker_api/#4-services-swarm-specific","text":"List services : Endpoint: GET /services bash curl -s --unix-socket /var/run/docker.sock http://localhost/services | jq Create a service : Endpoint: POST /services/create bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"Name\":\"myservice\", \"TaskTemplate\": {\"ContainerSpec\": {\"Image\":\"alpine\", \"Command\":[\"sleep\", \"3600\"]}}}' http://localhost/services/create | jq Inspect a service : Endpoint: GET /services/{id} bash curl -s --unix-socket /var/run/docker.sock http://localhost/services/{id} | jq Update a service : Endpoint: POST /services/{id}/update bash # This request needs additional details like the version of the service and the new configuration. Delete a service : Endpoint: DELETE /services/{id} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/services/{id}","title":"4. Services (Swarm Specific):"},{"location":"copy/docker/docker_api/#5-tasks-swarm-specific","text":"List tasks : Endpoint: GET /tasks bash curl -s --unix-socket /var/run/docker.sock http://localhost/tasks | jq Inspect a task : Endpoint: GET /tasks/{id} bash curl -s --unix-socket /var/run/docker.sock http://localhost/tasks/{id} | jq","title":"5. Tasks (Swarm Specific):"},{"location":"copy/docker/docker_api/#6-secrets-swarm-specific","text":"List secrets : Endpoint: GET /secrets bash curl -s --unix-socket /var/run/docker.sock http://localhost/secrets | jq Create a secret : Endpoint: POST /secrets/create bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"Name\":\"mysecret\", \"Data\":\"base64_encoded_data\"}' http://localhost/secrets/create | jq Inspect a secret : Endpoint: GET /secrets/{id} bash curl -s --unix-socket /var/run/docker.sock http://localhost/secrets/{id} | jq Delete a secret : Endpoint: DELETE /secrets/{id} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/secrets/{id}","title":"6. Secrets (Swarm Specific):"},{"location":"copy/docker/docker_api/#7-configs-swarm-specific","text":"List configs : Endpoint: GET /configs bash curl -s --unix-socket /var/run/docker.sock http://localhost/configs | jq Create a config : Endpoint: POST /configs/create bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"Name\":\"myconfig\", \"Data\":\"base64_encoded_data\"}' http://localhost/configs/create | jq Inspect a config : Endpoint: GET /configs/{id} bash curl -s --unix-socket /var/run/docker.sock http://localhost/configs/{id} | jq Delete a config : Endpoint: DELETE /configs/{id} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/configs/{id}","title":"7. Configs (Swarm Specific):"},{"location":"copy/docker/docker_api/#8-plugins","text":"List plugins : Endpoint: GET /plugins bash curl -s --unix-socket /var/run/docker.sock http://localhost/plugins | jq Inspect a plugin : Endpoint: GET /plugins/{name}/json bash curl -s --unix-socket /var/run/docker.sock http://localhost/plugins/{name}/json | jq Install a plugin : Endpoint: POST /plugins/pull bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"Name\":\"plugin_name\"}' http://localhost/plugins/pull Enable a plugin : Endpoint: POST /plugins/{name}/enable bash curl -s --unix-socket /var/run/docker.sock -X POST http://localhost/plugins/{name}/enable Disable a plugin : Endpoint: POST /plugins/{name}/disable bash curl -s --unix-socket /var/run/docker.sock -X POST http://localhost/plugins/{name}/disable Remove a plugin : Endpoint: DELETE /plugins/{name} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/plugins/{name}","title":"8. Plugins:"},{"location":"copy/docker/docker_api/#9-networks","text":"List networks : Endpoint: GET /networks bash curl -s --unix-socket /var/run/docker.sock http://localhost/networks | jq Inspect a network : Endpoint: GET /networks/{id} bash curl -s --unix-socket /var/run/docker.sock http://localhost/networks/{id} | jq Create a network : Endpoint: POST /networks/create bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"Name\":\"mynetwork\", \"Driver\":\"overlay\"}' http://localhost/networks/create | jq Remove a network : Endpoint: DELETE /networks/{id} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/networks/{id}","title":"9. Networks:"},{"location":"copy/docker/docker_api/#10-volumes","text":"List volumes : Endpoint: GET /volumes bash curl -s --unix-socket /var/run/docker.sock http://localhost/volumes | jq Inspect a volume : Endpoint: GET /volumes/{name} bash curl -s --unix-socket /var/run/docker.sock http://localhost/volumes/{name} | jq Create a volume : Endpoint: POST /volumes/create bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"Name\":\"myvolume\"}' http://localhost/volumes/create | jq Remove a volume : Endpoint: DELETE /volumes/{name} bash curl -s --unix-socket /var/run/docker.sock -X DELETE http://localhost/volumes/{name}","title":"10. Volumes:"},{"location":"copy/docker/docker_api/#11-system","text":"Get Docker version : Endpoint: GET /version bash curl -s --unix-socket /var/run/docker.sock http://localhost/version | jq Get system information : Endpoint: GET /info bash curl -s --unix-socket /var/run/docker.sock http://localhost/info | jq","title":"11. System:"},{"location":"copy/docker/docker_api/#12-exec","text":"Create exec instance : Endpoint: POST /containers/{id}/exec bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"AttachStdin\":false, \"AttachStdout\":true, \"AttachStderr\":true, \"Cmd\":[\"date\"]}' http://localhost/containers/{id}/exec | jq Start exec instance : Endpoint: POST /exec/{id}/start bash curl -s --unix-socket /var/run/docker.sock -X POST -H \"Content-Type: application/json\" -d '{\"Detach\": false, \"Tty\": false}' http://localhost/exec/{id}/start | jq","title":"12. Exec:"},{"location":"copy/docker/docker_api_in_container/","text":"Exposing Docker Daemon in Docker Swarm Nodes This document presents a solution to expose the Docker daemon on each node of a Docker Swarm cluster, allowing a management container to always interact with the Docker daemon of its current host node. Overview In a Docker Swarm setup, containers (or tasks) can be rescheduled or moved between nodes due to various reasons like node failures, scaling actions, or updates. For containers that need to interact with the Docker API, we aim to ensure they can always communicate with the Docker daemon of their host node. Solution 1. Global Docker API Proxy Service Goal Deploy a global service that exposes the Docker daemon of each node. Implementation Instead of using docker:dind (Docker in Docker), a lightweight proxy tool like socat will forward traffic from a container to the host's Docker socket. Docker Compose syntax for the service: services: docker-api-proxy: image: alpine/socat command: tcp-listen:2375,fork,reuseaddr unix-connect:/var/run/docker.sock deploy: mode: global 2. Management Container Goal Ensure the management container (e.g., a Node app) can communicate with the Docker daemon of its host node. Implementation No matter which node the management container is scheduled on, it can communicate with its local Docker daemon by addressing localhost:2375 . Important Considerations Swarm Mode Limitations Swarm mode has a set of API endpoints distinct from a regular Docker daemon. If the management app plans to manipulate Swarm-level features, it must handle these API differences. Security Exposing the Docker API can lead to security vulnerabilities: Even if exposed over localhost , there's a risk if a container manages to break its sandbox or if a malicious process runs on the node. Using TLS and client certificates can limit unauthorized external access, but local threats remain a concern. Direct Exposure vs. Nested Docker Daemon Using docker:dind means running a Docker daemon inside a Docker container, which has potential issues like data loss and performance concerns. Our chosen method directly connects the exposed TCP port in the container to the Docker socket on the host. Conclusion This setup ensures that a management container in a Docker Swarm environment can reliably and consistently communicate with the Docker daemon of its current host node. As always, careful attention to security and potential limitations is crucial.","title":"Docker API inside Container"},{"location":"copy/docker/docker_api_in_container/#exposing-docker-daemon-in-docker-swarm-nodes","text":"This document presents a solution to expose the Docker daemon on each node of a Docker Swarm cluster, allowing a management container to always interact with the Docker daemon of its current host node.","title":"Exposing Docker Daemon in Docker Swarm Nodes"},{"location":"copy/docker/docker_api_in_container/#overview","text":"In a Docker Swarm setup, containers (or tasks) can be rescheduled or moved between nodes due to various reasons like node failures, scaling actions, or updates. For containers that need to interact with the Docker API, we aim to ensure they can always communicate with the Docker daemon of their host node.","title":"Overview"},{"location":"copy/docker/docker_api_in_container/#solution","text":"","title":"Solution"},{"location":"copy/docker/docker_api_in_container/#1-global-docker-api-proxy-service","text":"","title":"1. Global Docker API Proxy Service"},{"location":"copy/docker/docker_api_in_container/#goal","text":"Deploy a global service that exposes the Docker daemon of each node.","title":"Goal"},{"location":"copy/docker/docker_api_in_container/#implementation","text":"Instead of using docker:dind (Docker in Docker), a lightweight proxy tool like socat will forward traffic from a container to the host's Docker socket. Docker Compose syntax for the service: services: docker-api-proxy: image: alpine/socat command: tcp-listen:2375,fork,reuseaddr unix-connect:/var/run/docker.sock deploy: mode: global","title":"Implementation"},{"location":"copy/docker/docker_api_in_container/#2-management-container","text":"","title":"2. Management Container"},{"location":"copy/docker/docker_api_in_container/#goal_1","text":"Ensure the management container (e.g., a Node app) can communicate with the Docker daemon of its host node.","title":"Goal"},{"location":"copy/docker/docker_api_in_container/#implementation_1","text":"No matter which node the management container is scheduled on, it can communicate with its local Docker daemon by addressing localhost:2375 .","title":"Implementation"},{"location":"copy/docker/docker_api_in_container/#important-considerations","text":"","title":"Important Considerations"},{"location":"copy/docker/docker_api_in_container/#swarm-mode-limitations","text":"Swarm mode has a set of API endpoints distinct from a regular Docker daemon. If the management app plans to manipulate Swarm-level features, it must handle these API differences.","title":"Swarm Mode Limitations"},{"location":"copy/docker/docker_api_in_container/#security","text":"Exposing the Docker API can lead to security vulnerabilities: Even if exposed over localhost , there's a risk if a container manages to break its sandbox or if a malicious process runs on the node. Using TLS and client certificates can limit unauthorized external access, but local threats remain a concern.","title":"Security"},{"location":"copy/docker/docker_api_in_container/#direct-exposure-vs-nested-docker-daemon","text":"Using docker:dind means running a Docker daemon inside a Docker container, which has potential issues like data loss and performance concerns. Our chosen method directly connects the exposed TCP port in the container to the Docker socket on the host.","title":"Direct Exposure vs. Nested Docker Daemon"},{"location":"copy/docker/docker_api_in_container/#conclusion","text":"This setup ensures that a management container in a Docker Swarm environment can reliably and consistently communicate with the Docker daemon of its current host node. As always, careful attention to security and potential limitations is crucial.","title":"Conclusion"},{"location":"copy/git/move_changes_to_another_branch/","text":"Moving Changes to a Different Branch If you've made changes to a branch and realized that you should have made those changes on a different branch, follow these steps to move your changes without losing them: 1. Stash Your Changes First, stash your changes so that you can apply them to another branch. git stash 2. Create or Checkout the Desired Branch If your desired branch doesn't exist yet, create and checkout the branch: git checkout -b [branch_name] If the branch already exists, simply checkout: git checkout [branch_name] 3. Apply Your Stashed Changes Now, apply the changes you stashed: git stash apply 4. Commit Your Changes Once you've applied your changes to the correct branch, commit them: git add . git commit -m \"Your commit message\" 5. Optional - Clean Up If everything looks good and you've committed your changes, drop the stash: git stash drop Note : Always double-check which branch you're on before making changes. To push these changes to a remote repository: git push origin [branch_name]","title":"Moving Changes to a Different Branch"},{"location":"copy/git/move_changes_to_another_branch/#moving-changes-to-a-different-branch","text":"If you've made changes to a branch and realized that you should have made those changes on a different branch, follow these steps to move your changes without losing them:","title":"Moving Changes to a Different Branch"},{"location":"copy/git/move_changes_to_another_branch/#1-stash-your-changes","text":"First, stash your changes so that you can apply them to another branch. git stash","title":"1. Stash Your Changes"},{"location":"copy/git/move_changes_to_another_branch/#2-create-or-checkout-the-desired-branch","text":"If your desired branch doesn't exist yet, create and checkout the branch: git checkout -b [branch_name] If the branch already exists, simply checkout: git checkout [branch_name]","title":"2. Create or Checkout the Desired Branch"},{"location":"copy/git/move_changes_to_another_branch/#3-apply-your-stashed-changes","text":"Now, apply the changes you stashed: git stash apply","title":"3. Apply Your Stashed Changes"},{"location":"copy/git/move_changes_to_another_branch/#4-commit-your-changes","text":"Once you've applied your changes to the correct branch, commit them: git add . git commit -m \"Your commit message\"","title":"4. Commit Your Changes"},{"location":"copy/git/move_changes_to_another_branch/#5-optional-clean-up","text":"If everything looks good and you've committed your changes, drop the stash: git stash drop Note : Always double-check which branch you're on before making changes. To push these changes to a remote repository: git push origin [branch_name]","title":"5. Optional - Clean Up"},{"location":"data/managed/","text":"Managed Database API This documentation provides information on how to use the API endpoints provided by your managed database. The API allows users to interact with a database, perform operations such as storing data, retrieving data, listing collections, and more. API Documentation This documentation provides information on how to use the API endpoints defined in the provided code. The API allows users to interact with a database, perform operations such as storing data, retrieving data, listing collections, and more. Important: All communications with the API must be done using JSON format and Base64 encoding. Authentication Before making requests to the API, you need to obtain an access token by sending a POST request to the authentication endpoint. The following example demonstrates how to obtain an access token: token=$(curl -X POST -d \"grant_type=client_credentials&client_id=<client_id>&client_secret=<client_secret>\" \\ https://sso.sexycoders.org/auth/realms/sexycoders.org/protocol/openid-connect/token | jq -r '.access_token') Replace <client_id> and <client_secret> with your actual credentials. Storing Data To store data in the database, send a POST request to the /storeData endpoint. Include the access token in the request header and the data in the request body as an object. The example below demonstrates how to store data: body='{\"database\":\"<database_name>\",\"collection\":\"<collection_name>\",\"data\":{\"<field_name>\":\"<value>\",\"<field_name>\":\"<value>\"}}' base64_body=$(echo -n \"$body\" | base64) curl https://viki.sexycoders.org/api/data/storeData \\ -H \"Authorization: Bearer $token\" \\ -H \"Content-Type: application/json\" \\ -d \"$base64_body\" Replace <database_name> with the name of the database, <collection_name> with the name of the collection, <field_name> with the actual field names in your data, and <value> with the corresponding values. Note: The /store endpoint expects the data field to be a single object. Storing Multiple Documents To store multiple documents in the database, send a POST request to the /storeMultiple endpoint. Include the access token in the request header and the data in the request body as a nested array of objects. The example below demonstrates how to store multiple documents: body='{\"database\":\"<database_name>\",\"collection\":\"<collection_name>\",\"data\":[{\"<field_name>\":\"<value>\",\"<field_name>\":\"<value>\"}, {\"<field_name>\":\"<value>\",\"<field_name>\":\"<value>\"}]}' base64_body=$(echo -n \"$body\" | base64) curl https://viki.sexycoders.org/api/data/storeMultiple \\ -H \"Authorization: Bearer $token\" \\ -H \"Content-Type: application/json\" \\ -d \"$base64_body\" Replace <database_name> with the name of the database, <collection_name> with the name of the collection, <field_name> with the actual field names in your data, and <value> with the corresponding values. Note: The /storeMultiple endpoint expects the data field to be an array of objects. Retrieving Data To retrieve data from the database, send a POST request to the /retrieveData endpoint. Include the access token in the request header and specify the database, collection, and optional count or constraints in the request body. The example below demonstrates how to retrieve data: body='{\"database\":\"<database_name>\",\"collection\":\"<collection_name>\",\"constraints\":{\"<field_name>\":\"<value>\"},\"count\":<count>}' base64_body=$(echo -n \"$body\" | base64) curl https://viki.sexycoders.org/api/data/retrieveData \\ -H \"Authorization: Bearer $token\" \\ -H \"Content-Type: application/json\" \\ -d \"$base64_body\" Replace <database_name> with the name of the database, <collection_name> with the name of the collection, <field_name> with the actual field names in your data, <value> with the corresponding values, and <count> with the desired number of documents to retrieve (optional). Listing Collections To list collections in the database, send a POST request to the /listCollections endpoint. Include the access token in the request header and specify the database in the request body. The example below demonstrates how to list collections: body='{\"database\":\"<database_name>\"}' base64_body=$(echo -n \"$body\" | base64) curl https://viki.sexycoders.org/api/data/listCollections \\ -H \"Authorization: Bearer $token\" \\ -H \"Content-Type: application/json\" \\ -d \"$base64_body\" Replace <database_name> with the name of the database. Note: Make sure to replace the placeholders <client_id> , <client_secret> , <database_name> , <collection_name> , <field_name> , and <value> with the actual values specific to your setup. Remember to include the access token obtained through the authentication process in the request headers when interacting with the API. For further assistance or inquiries, please contact the API support team at support@example.com.","title":"Managed Databases"},{"location":"data/managed/#managed-database-api","text":"This documentation provides information on how to use the API endpoints provided by your managed database. The API allows users to interact with a database, perform operations such as storing data, retrieving data, listing collections, and more.","title":"Managed Database API"},{"location":"data/managed/#api-documentation","text":"This documentation provides information on how to use the API endpoints defined in the provided code. The API allows users to interact with a database, perform operations such as storing data, retrieving data, listing collections, and more. Important: All communications with the API must be done using JSON format and Base64 encoding.","title":"API Documentation"},{"location":"data/managed/#authentication","text":"Before making requests to the API, you need to obtain an access token by sending a POST request to the authentication endpoint. The following example demonstrates how to obtain an access token: token=$(curl -X POST -d \"grant_type=client_credentials&client_id=<client_id>&client_secret=<client_secret>\" \\ https://sso.sexycoders.org/auth/realms/sexycoders.org/protocol/openid-connect/token | jq -r '.access_token') Replace <client_id> and <client_secret> with your actual credentials.","title":"Authentication"},{"location":"data/managed/#storing-data","text":"To store data in the database, send a POST request to the /storeData endpoint. Include the access token in the request header and the data in the request body as an object. The example below demonstrates how to store data: body='{\"database\":\"<database_name>\",\"collection\":\"<collection_name>\",\"data\":{\"<field_name>\":\"<value>\",\"<field_name>\":\"<value>\"}}' base64_body=$(echo -n \"$body\" | base64) curl https://viki.sexycoders.org/api/data/storeData \\ -H \"Authorization: Bearer $token\" \\ -H \"Content-Type: application/json\" \\ -d \"$base64_body\" Replace <database_name> with the name of the database, <collection_name> with the name of the collection, <field_name> with the actual field names in your data, and <value> with the corresponding values. Note: The /store endpoint expects the data field to be a single object.","title":"Storing Data"},{"location":"data/managed/#storing-multiple-documents","text":"To store multiple documents in the database, send a POST request to the /storeMultiple endpoint. Include the access token in the request header and the data in the request body as a nested array of objects. The example below demonstrates how to store multiple documents: body='{\"database\":\"<database_name>\",\"collection\":\"<collection_name>\",\"data\":[{\"<field_name>\":\"<value>\",\"<field_name>\":\"<value>\"}, {\"<field_name>\":\"<value>\",\"<field_name>\":\"<value>\"}]}' base64_body=$(echo -n \"$body\" | base64) curl https://viki.sexycoders.org/api/data/storeMultiple \\ -H \"Authorization: Bearer $token\" \\ -H \"Content-Type: application/json\" \\ -d \"$base64_body\" Replace <database_name> with the name of the database, <collection_name> with the name of the collection, <field_name> with the actual field names in your data, and <value> with the corresponding values. Note: The /storeMultiple endpoint expects the data field to be an array of objects.","title":"Storing Multiple Documents"},{"location":"data/managed/#retrieving-data","text":"To retrieve data from the database, send a POST request to the /retrieveData endpoint. Include the access token in the request header and specify the database, collection, and optional count or constraints in the request body. The example below demonstrates how to retrieve data: body='{\"database\":\"<database_name>\",\"collection\":\"<collection_name>\",\"constraints\":{\"<field_name>\":\"<value>\"},\"count\":<count>}' base64_body=$(echo -n \"$body\" | base64) curl https://viki.sexycoders.org/api/data/retrieveData \\ -H \"Authorization: Bearer $token\" \\ -H \"Content-Type: application/json\" \\ -d \"$base64_body\" Replace <database_name> with the name of the database, <collection_name> with the name of the collection, <field_name> with the actual field names in your data, <value> with the corresponding values, and <count> with the desired number of documents to retrieve (optional).","title":"Retrieving Data"},{"location":"data/managed/#listing-collections","text":"To list collections in the database, send a POST request to the /listCollections endpoint. Include the access token in the request header and specify the database in the request body. The example below demonstrates how to list collections: body='{\"database\":\"<database_name>\"}' base64_body=$(echo -n \"$body\" | base64) curl https://viki.sexycoders.org/api/data/listCollections \\ -H \"Authorization: Bearer $token\" \\ -H \"Content-Type: application/json\" \\ -d \"$base64_body\" Replace <database_name> with the name of the database. Note: Make sure to replace the placeholders <client_id> , <client_secret> , <database_name> , <collection_name> , <field_name> , and <value> with the actual values specific to your setup. Remember to include the access token obtained through the authentication process in the request headers when interacting with the API. For further assistance or inquiries, please contact the API support team at support@example.com.","title":"Listing Collections"},{"location":"for_developers/helpers/","text":"Helper Commands To automate and simplify many everyday tasks, our team has created and maintains a variety of helpers under /bin in the SexyCoders main repository. sc-docker-build The docker build script builds docker images using the secycoders naming convention and automates versioning. It also provides additionall options to change the name, push to the registry and include the \"latest\" tag when pushing for production. We can see the available commands using the \"--help\" flag: Usage: ./sc-docker-build [OPTIONS] Options: -h, --help Show this help message and exit -p, --push Optionally push the image to the registry -l, --latest Additionally tag the image as 'latest' -n, --name NAME The name to be used for the Docker image (optional) The name is determined in the following oder: \"--name\" flag is provided name is read from local \"name\" file name is generated from current dir name \u26a0 In case a --name flag is provided but a name file already exists you will be prompted on whether to overwrite.","title":"Helper Commands"},{"location":"for_developers/helpers/#helper-commands","text":"To automate and simplify many everyday tasks, our team has created and maintains a variety of helpers under /bin in the SexyCoders main repository.","title":"Helper Commands"},{"location":"for_developers/helpers/#sc-docker-build","text":"The docker build script builds docker images using the secycoders naming convention and automates versioning. It also provides additionall options to change the name, push to the registry and include the \"latest\" tag when pushing for production. We can see the available commands using the \"--help\" flag: Usage: ./sc-docker-build [OPTIONS] Options: -h, --help Show this help message and exit -p, --push Optionally push the image to the registry -l, --latest Additionally tag the image as 'latest' -n, --name NAME The name to be used for the Docker image (optional) The name is determined in the following oder: \"--name\" flag is provided name is read from local \"name\" file name is generated from current dir name \u26a0 In case a --name flag is provided but a name file already exists you will be prompted on whether to overwrite.","title":"sc-docker-build"},{"location":"for_developers/versioning/","text":"Naming Conventions and Versioning Since the SexyCoders ecosystem is an open source and community based project, we have implemented and are strictly abiding to naming conventions so that everyone can follow. Please follow them or you will risk your work not being approved by our review team. Helpers To help navigate and automate these and many other proccesses we have created a set of commands under /bin the SexyCoders main repository. It is suggested to use those instead! Find documentation on them here . If you still wish to do it by hand read on. Please also look in the individual dirs for \".format\" files that will direct you further for the specifics of each service (like name conventions, reserved naems, legacy compatibillity issues etc.) General Format The general date format for versioning is 14-05-2023_1231_EEST and can be produced using the following: date +'%d-%m-%Y_%H%M_%Z' This date format was introduced in March 2023 and applied to all aspects of the sexycoders ecosystem to create a unified approach to versioning. We expect it to have fully replaced any old versioning within a couple of months. Docker For docker please use vXX_$(date +'%d-%m-%Y_%H%M_%Z') for example if building \"example_image\" and latest version is 12 do: docker build -t registry.sexycoders.org/example_image:v12_$(date +'%d-%m-%Y_%H%M_%Z') Git Branches For git branches use a similar approach : <branch_name>_$(date +'%d-%m-%Y_%H%M_%Z') For example to create a branch named new_branch from the master: git checkout -B new_branch_$(date +'%d-%m-%Y_%H%M_%Z')","title":"Names and Versioning"},{"location":"for_developers/versioning/#naming-conventions-and-versioning","text":"Since the SexyCoders ecosystem is an open source and community based project, we have implemented and are strictly abiding to naming conventions so that everyone can follow. Please follow them or you will risk your work not being approved by our review team.","title":"Naming Conventions and Versioning"},{"location":"for_developers/versioning/#helpers","text":"To help navigate and automate these and many other proccesses we have created a set of commands under /bin the SexyCoders main repository. It is suggested to use those instead! Find documentation on them here . If you still wish to do it by hand read on. Please also look in the individual dirs for \".format\" files that will direct you further for the specifics of each service (like name conventions, reserved naems, legacy compatibillity issues etc.)","title":"Helpers"},{"location":"for_developers/versioning/#general-format","text":"The general date format for versioning is 14-05-2023_1231_EEST and can be produced using the following: date +'%d-%m-%Y_%H%M_%Z' This date format was introduced in March 2023 and applied to all aspects of the sexycoders ecosystem to create a unified approach to versioning. We expect it to have fully replaced any old versioning within a couple of months.","title":"General Format"},{"location":"for_developers/versioning/#docker","text":"For docker please use vXX_$(date +'%d-%m-%Y_%H%M_%Z') for example if building \"example_image\" and latest version is 12 do: docker build -t registry.sexycoders.org/example_image:v12_$(date +'%d-%m-%Y_%H%M_%Z')","title":"Docker"},{"location":"for_developers/versioning/#git-branches","text":"For git branches use a similar approach : <branch_name>_$(date +'%d-%m-%Y_%H%M_%Z') For example to create a branch named new_branch from the master: git checkout -B new_branch_$(date +'%d-%m-%Y_%H%M_%Z')","title":"Git Branches"},{"location":"thirdparty/solar/","text":"API Documentation Introduction Welcome to the documentation for the Microsun Solar API. This API allows authorized third-party applications to access and retrieve data related to inverters. To access the API, you need to obtain an access token by following the authentication process outlined below. If you dont have credentials and need help getting them please contact us. Authentication All endpoints in this API require authentication using an access token. To obtain an access token please read through the Authentication guide. WARNING As always with our apis the respones are base64 and JSON encoded and the requests are expected to be as well! Endpoints 1. Get Park Inverter Logs Endpoint: /api/thirdparty/getParkInverterLogs Method: POST Authorization: Required (Bearer token) Request Body: The request body must be a JSON object containing the following parameter: parkId (string, required): The unique identifier of the park for which you want to retrieve inverter logs. Example using curl : body='{\"parkId\":\"8\"}' base64_body=$(echo -n \"$body\" | base64) curl https://viki.sexycoders.org/api/thirdparty/getParkInverterLogs \\ -H \"Authorization: Bearer $token\" \\ -d \"$base64_body\" Response: If successful, the response will be a JSON object containing the data for all inverters belonging to the specified park. 200 OK with the response data on success. 400 Bad Request if parkId is missing in the request body. 404 Not Found if no inverters are found for the specified park. 500 Internal Server Error on server errors. 2. Get Inverter Data by ID Endpoint: /api/thirdparty/getInverterDataById Method: POST Authorization: Required (Bearer token) Request Body: The request body must be a JSON object containing the following parameter: inverterId (string, required): The unique identifier of the inverter for which you want to retrieve data. Example using curl : body='{\"inverterId\":\"xyz123\"}' base64_body=$(echo -n \"$body\" | base64) curl https://viki.sexycoders.org/api/thirdparty/getInverterDataById \\ -H \"Authorization: Bearer $token\" \\ -d \"$base64_body\" Response: If successful, the response will be a JSON object containing the data for the specified inverter. 200 OK with the response data on success. 404 Not Found if the specified inverter is not found. 500 Internal Server Error on server errors. 3. Get Combined Inverter Data Endpoint: /api/thirdparty/getCombinedInverterData Method: POST Authorization: Required (Bearer token) Request Body: This endpoint does not require any request body. Example using curl : curl https://viki.sexycoders.org/api/thirdparty/getCombinedInverterData \\ -H \"Authorization: Bearer $token\" Response: If successful, the response will be a JSON object containing the combined data for all available inverters. 200 OK with the response data on success. 500 Internal Server Error on server errors. Error Handling In case of errors, the API will return appropriate HTTP status codes along with JSON error messages in the response body. Make sure to handle these error responses in your application. Rate Limiting This API does not have rate limiting implemented at this time. However, it is recommended to avoid excessive requests to ensure fair usage and optimal performance for all users and encourage us to keep it free. Happy coding! Solar Plant Data provided by microsun energy systems","title":"Solar API"},{"location":"thirdparty/solar/#api-documentation","text":"","title":"API Documentation"},{"location":"thirdparty/solar/#introduction","text":"Welcome to the documentation for the Microsun Solar API. This API allows authorized third-party applications to access and retrieve data related to inverters. To access the API, you need to obtain an access token by following the authentication process outlined below. If you dont have credentials and need help getting them please contact us.","title":"Introduction"},{"location":"thirdparty/solar/#authentication","text":"All endpoints in this API require authentication using an access token. To obtain an access token please read through the Authentication guide.","title":"Authentication"},{"location":"thirdparty/solar/#warning","text":"As always with our apis the respones are base64 and JSON encoded and the requests are expected to be as well!","title":"WARNING"},{"location":"thirdparty/solar/#endpoints","text":"","title":"Endpoints"},{"location":"thirdparty/solar/#1-get-park-inverter-logs","text":"Endpoint: /api/thirdparty/getParkInverterLogs Method: POST Authorization: Required (Bearer token) Request Body: The request body must be a JSON object containing the following parameter: parkId (string, required): The unique identifier of the park for which you want to retrieve inverter logs. Example using curl : body='{\"parkId\":\"8\"}' base64_body=$(echo -n \"$body\" | base64) curl https://viki.sexycoders.org/api/thirdparty/getParkInverterLogs \\ -H \"Authorization: Bearer $token\" \\ -d \"$base64_body\" Response: If successful, the response will be a JSON object containing the data for all inverters belonging to the specified park. 200 OK with the response data on success. 400 Bad Request if parkId is missing in the request body. 404 Not Found if no inverters are found for the specified park. 500 Internal Server Error on server errors.","title":"1. Get Park Inverter Logs"},{"location":"thirdparty/solar/#2-get-inverter-data-by-id","text":"Endpoint: /api/thirdparty/getInverterDataById Method: POST Authorization: Required (Bearer token) Request Body: The request body must be a JSON object containing the following parameter: inverterId (string, required): The unique identifier of the inverter for which you want to retrieve data. Example using curl : body='{\"inverterId\":\"xyz123\"}' base64_body=$(echo -n \"$body\" | base64) curl https://viki.sexycoders.org/api/thirdparty/getInverterDataById \\ -H \"Authorization: Bearer $token\" \\ -d \"$base64_body\" Response: If successful, the response will be a JSON object containing the data for the specified inverter. 200 OK with the response data on success. 404 Not Found if the specified inverter is not found. 500 Internal Server Error on server errors.","title":"2. Get Inverter Data by ID"},{"location":"thirdparty/solar/#3-get-combined-inverter-data","text":"Endpoint: /api/thirdparty/getCombinedInverterData Method: POST Authorization: Required (Bearer token) Request Body: This endpoint does not require any request body. Example using curl : curl https://viki.sexycoders.org/api/thirdparty/getCombinedInverterData \\ -H \"Authorization: Bearer $token\" Response: If successful, the response will be a JSON object containing the combined data for all available inverters. 200 OK with the response data on success. 500 Internal Server Error on server errors.","title":"3. Get Combined Inverter Data"},{"location":"thirdparty/solar/#error-handling","text":"In case of errors, the API will return appropriate HTTP status codes along with JSON error messages in the response body. Make sure to handle these error responses in your application.","title":"Error Handling"},{"location":"thirdparty/solar/#rate-limiting","text":"This API does not have rate limiting implemented at this time. However, it is recommended to avoid excessive requests to ensure fair usage and optimal performance for all users and encourage us to keep it free. Happy coding! Solar Plant Data provided by microsun energy systems","title":"Rate Limiting"},{"location":"thirdparty/weather/","text":"Weather API Documentation Introduction This API provides various endpoints to fetch weather and irradiance information for specific parks. To access the API, you need to obtain an access token by following the authentication process outlined below. If you don't have credentials and need help getting them, please contact us. Authentication All endpoints in this API require authentication using an access token. To obtain an access token, please read through the Authentication guide. WARNING As always with our APIs, the responses are base64 and JSON encoded, and the requests are expected to be as well! Endpoints 1. Get Park Irradiance Information Endpoint: /api/thirdparty/getParkIrradianceInfo Method: POST Description: Fetch irradiance information for a given park. Body: parkId : (required) The ID of the park. tz : (required) Timezone for the irradiance data. date : (required) Date for which irradiance data is required. Example: body='{\"parkId\":\"7\", \"tz\": \"+03:00\", \"date\": \"2023-08-09\"}' base64_body=$(echo -n \"$body\" | base64) curl https://viki.sexycoders.org/api/thirdparty/getParkIrradianceInfo \\ -H \"Authorization: Bearer $token\" \\ -d \"$base64_body\" 2. Get Full Weather Information for Park Endpoint: /api/thirdparty/getParkWeatherInfoFull Method: POST Description: Fetch detailed weather information for a specific park. Body: parkId : (required) The ID of the park. Example: body='{\"parkId\":\"7\"}' base64_body=$(echo -n \"$body\" | base64) curl https://viki.sexycoders.org/api/thirdparty/getParkWeatherInfoFull \\ -H \"Authorization: Bearer $token\" \\ -d \"$base64_body\" 3. Get Park Weather Information by Timestamp Endpoint: /api/thirdparty/getParkWeatherInfoByTimestamp Method: POST Description: Fetch weather information for a specific park at a given timestamp. Body: parkId : (required) The ID of the park. dt : (required) Unix timestamp for which weather data is required. Example: body='{\"parkId\":\"7\", \"dt\": \"1677897600\"}' # Replace with desired timestamp base64_body=$(echo -n \"$body\" | base64) curl https://viki.sexycoders.org/api/thirdparty/getParkWeatherInfoByTimestamp \\ -H \"Authorization: Bearer $token\" \\ -d \"$base64_body\" 4. Get Basic Weather Information for Park Endpoint: /api/thirdparty/getParkWeatherInfo Method: POST Description: Fetch basic weather information for a specific park. Body: parkId : (required) The ID of the park. Example: body='{\"parkId\":\"7\"}' base64_body=$(echo -n \"$body\" | base64) curl https://viki.sexycoders.org/api/thirdparty/getParkWeatherInfo \\ -H \"Authorization: Bearer $token\" \\ -d \"$base64_body\" Error Handling If there's an issue with the request, the server will respond with an error in the format: { \"error\": \"Error message\" } . Common errors include: Missing parkId in the request body. Invalid parkId . Internal Server Error. Weather data provided by OpenWeather","title":"Weather API"},{"location":"thirdparty/weather/#weather-api-documentation","text":"","title":"Weather API Documentation"},{"location":"thirdparty/weather/#introduction","text":"This API provides various endpoints to fetch weather and irradiance information for specific parks. To access the API, you need to obtain an access token by following the authentication process outlined below. If you don't have credentials and need help getting them, please contact us.","title":"Introduction"},{"location":"thirdparty/weather/#authentication","text":"All endpoints in this API require authentication using an access token. To obtain an access token, please read through the Authentication guide.","title":"Authentication"},{"location":"thirdparty/weather/#warning","text":"As always with our APIs, the responses are base64 and JSON encoded, and the requests are expected to be as well!","title":"WARNING"},{"location":"thirdparty/weather/#endpoints","text":"","title":"Endpoints"},{"location":"thirdparty/weather/#1-get-park-irradiance-information","text":"Endpoint: /api/thirdparty/getParkIrradianceInfo Method: POST Description: Fetch irradiance information for a given park. Body: parkId : (required) The ID of the park. tz : (required) Timezone for the irradiance data. date : (required) Date for which irradiance data is required. Example: body='{\"parkId\":\"7\", \"tz\": \"+03:00\", \"date\": \"2023-08-09\"}' base64_body=$(echo -n \"$body\" | base64) curl https://viki.sexycoders.org/api/thirdparty/getParkIrradianceInfo \\ -H \"Authorization: Bearer $token\" \\ -d \"$base64_body\"","title":"1. Get Park Irradiance Information"},{"location":"thirdparty/weather/#2-get-full-weather-information-for-park","text":"Endpoint: /api/thirdparty/getParkWeatherInfoFull Method: POST Description: Fetch detailed weather information for a specific park. Body: parkId : (required) The ID of the park. Example: body='{\"parkId\":\"7\"}' base64_body=$(echo -n \"$body\" | base64) curl https://viki.sexycoders.org/api/thirdparty/getParkWeatherInfoFull \\ -H \"Authorization: Bearer $token\" \\ -d \"$base64_body\"","title":"2. Get Full Weather Information for Park"},{"location":"thirdparty/weather/#3-get-park-weather-information-by-timestamp","text":"Endpoint: /api/thirdparty/getParkWeatherInfoByTimestamp Method: POST Description: Fetch weather information for a specific park at a given timestamp. Body: parkId : (required) The ID of the park. dt : (required) Unix timestamp for which weather data is required. Example: body='{\"parkId\":\"7\", \"dt\": \"1677897600\"}' # Replace with desired timestamp base64_body=$(echo -n \"$body\" | base64) curl https://viki.sexycoders.org/api/thirdparty/getParkWeatherInfoByTimestamp \\ -H \"Authorization: Bearer $token\" \\ -d \"$base64_body\"","title":"3. Get Park Weather Information by Timestamp"},{"location":"thirdparty/weather/#4-get-basic-weather-information-for-park","text":"Endpoint: /api/thirdparty/getParkWeatherInfo Method: POST Description: Fetch basic weather information for a specific park. Body: parkId : (required) The ID of the park. Example: body='{\"parkId\":\"7\"}' base64_body=$(echo -n \"$body\" | base64) curl https://viki.sexycoders.org/api/thirdparty/getParkWeatherInfo \\ -H \"Authorization: Bearer $token\" \\ -d \"$base64_body\"","title":"4. Get Basic Weather Information for Park"},{"location":"thirdparty/weather/#error-handling","text":"If there's an issue with the request, the server will respond with an error in the format: { \"error\": \"Error message\" } . Common errors include: Missing parkId in the request body. Invalid parkId . Internal Server Error. Weather data provided by OpenWeather","title":"Error Handling"},{"location":"tools/apis/","text":"APIs, Node.js and Express.js Documentation Introduction This documentation is intended to provide a comprehensive understanding of how APIs, Node.js, and Express.js are utilized in modern cloud solutions. The goal is to shed light on why these technologies have become pivotal tools for developers working with cloud infrastructure. What is an API? An Application Programming Interface (API) is a set of rules and protocols for building and interacting with software applications. In the context of cloud computing, APIs enable the interaction between different software modules and components, often operating across distributed systems. Node.js in Cloud Node.js has become a favored technology in the cloud for several reasons. Its non-blocking, event-driven architecture makes it particularly well-suited for cloud applications which often need to handle a large number of simultaneous connections with high efficiency. Moreover, Node.js's JavaScript basis makes it universal and easy to adopt. Express.js in Cloud Express.js, often used in conjunction with Node.js, is a lightweight, fast, unopinionated web application framework that provides a simple API for building scalable server-side applications. In the cloud, Express.js allows for quick development of microservices, APIs, and other server-side logic, which can be deployed and scaled independently. Modern Cloud Solutions with Node.js and Express.js Modern cloud solutions leverage the power of Node.js and Express.js to build scalable and resilient systems. They help create applications that are able to communicate seamlessly with various services, and efficiently handle large numbers of connections. Moreover, the simplicity and minimalism of Express.js make it ideal for microservice architectures, which are now a standard in modern cloud solutions. Conclusion In this era of digital transformation, Node.js and Express.js have emerged as key technologies in the world of cloud computing. By making server-side development simpler, more efficient, and scalable, these technologies are powering the backbone of many modern cloud solutions. Read More General Mozilla Developer Network - API Basics NodeJS Node.js Official Documentation What Exactly is Node.js and Why Should You Use It? About Node.js Node.js Development: A Comprehensive Guide 7 Reasons Why the Popularity of Node.js has Increased Immensely Express Express.js Official Documentation What is Express.js? What is Express.js? Benefits of Using Express.js for Backend Development","title":"APIs"},{"location":"tools/apis/#apis-nodejs-and-expressjs-documentation","text":"","title":"APIs, Node.js and Express.js Documentation"},{"location":"tools/apis/#introduction","text":"This documentation is intended to provide a comprehensive understanding of how APIs, Node.js, and Express.js are utilized in modern cloud solutions. The goal is to shed light on why these technologies have become pivotal tools for developers working with cloud infrastructure.","title":"Introduction "},{"location":"tools/apis/#what-is-an-api","text":"An Application Programming Interface (API) is a set of rules and protocols for building and interacting with software applications. In the context of cloud computing, APIs enable the interaction between different software modules and components, often operating across distributed systems.","title":"What is an API? "},{"location":"tools/apis/#nodejs-in-cloud","text":"Node.js has become a favored technology in the cloud for several reasons. Its non-blocking, event-driven architecture makes it particularly well-suited for cloud applications which often need to handle a large number of simultaneous connections with high efficiency. Moreover, Node.js's JavaScript basis makes it universal and easy to adopt.","title":"Node.js in Cloud "},{"location":"tools/apis/#expressjs-in-cloud","text":"Express.js, often used in conjunction with Node.js, is a lightweight, fast, unopinionated web application framework that provides a simple API for building scalable server-side applications. In the cloud, Express.js allows for quick development of microservices, APIs, and other server-side logic, which can be deployed and scaled independently.","title":"Express.js in Cloud "},{"location":"tools/apis/#modern-cloud-solutions-with-nodejs-and-expressjs","text":"Modern cloud solutions leverage the power of Node.js and Express.js to build scalable and resilient systems. They help create applications that are able to communicate seamlessly with various services, and efficiently handle large numbers of connections. Moreover, the simplicity and minimalism of Express.js make it ideal for microservice architectures, which are now a standard in modern cloud solutions.","title":"Modern Cloud Solutions with Node.js and Express.js "},{"location":"tools/apis/#conclusion","text":"In this era of digital transformation, Node.js and Express.js have emerged as key technologies in the world of cloud computing. By making server-side development simpler, more efficient, and scalable, these technologies are powering the backbone of many modern cloud solutions.","title":"Conclusion "},{"location":"tools/apis/#read-more","text":"","title":"Read More"},{"location":"tools/apis/#general","text":"Mozilla Developer Network - API Basics","title":"General"},{"location":"tools/apis/#nodejs","text":"Node.js Official Documentation What Exactly is Node.js and Why Should You Use It? About Node.js Node.js Development: A Comprehensive Guide 7 Reasons Why the Popularity of Node.js has Increased Immensely","title":"NodeJS"},{"location":"tools/apis/#express","text":"Express.js Official Documentation What is Express.js? What is Express.js? Benefits of Using Express.js for Backend Development","title":"Express"},{"location":"tools/docker/","text":"Docker and Docker Swarm Introduction to Docker Docker is an open-source platform designed to automate the deployment, scaling, and operation of applications. It uses containerization technology to wrap software and its dependencies into a standardized unit for software development. Docker containers, which are lightweight and standalone, can run on any machine that installs Docker, regardless of the operating environment. Why Use Docker? Docker brings several benefits to the software development process: Portability : Since Docker containers encapsulate everything an application needs to run (including the operating system, application code, runtime, system tools, and libraries), they ensure consistency across multiple development and staging environments. Efficiency : Docker containers are lightweight because they use shared operating systems. They take up less space than VMs (virtual machines), and start up and replicate quickly. Version Control and Component Reuse : Docker has built-in version control capabilities that make it easy to track changes, roll back, and ensure consistency. Sharing : Docker has a public registry (Docker Hub) where you can share your containers with others, which is particularly useful for publishing your work and collaborating with others. Docker Compose Docker Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application's services, and then with a single command, you can create and start all the services from your configuration. Using Docker Compose can simplify the process of managing multi-container Docker applications. It allows developers to define an application's infrastructure and dependencies in a single file, then spin up the application with a single command. Introduction to Docker Swarm Docker Swarm, often just referred to as Swarm, is Docker's native clustering and scheduling tool. A swarm is a group of machines that are running Docker and joined into a cluster. After that has been established, you continue to run the Docker commands you're used to, but now they are executed on a cluster by a swarm manager. Swarm uses the standard Docker API, so any tool that already communicates with a Docker daemon can use Swarm to transparently scale to multiple hosts. Why Use Docker Swarm? Docker Swarm provides several advantages: Distributed Design : Instead of handling everything on one single Docker host, you can handle it across multiple Docker hosts. Scalability : Docker Swarm allows you to increase the number of container instances as demand increases. High Availability : Docker Swarm provides high availability with features like service replication and service health checking. Security : Docker Swarm uses TLS for authentication, role-based access control, and cryptographic node identity, making it a secure choice for deployment. Docker in Modern Cloud Solutions Docker is increasingly used in modern cloud solutions for several reasons: Microservices Architecture : Docker is ideal for microservices architecture because it allows each service to run in its own container, making it easy to scale and update services independently. Continuous Integration/Continuous Deployment (CI/CD) : Docker can integrate with popular CI/CD tools like Jenkins, allowing for seamless, efficient, and consistent delivery pipelines. DevOps Practices : Docker aligns with DevOps practices by enabling developers to work in standardized environments using local containers which provide your applications and services. This greatly reduces the \"it works on my machine\" problem. Cloud Portability : Applications packaged as Docker containers can run on any machine that has Docker installed, regardless of the underlying infrastructure. This makes it easy to move applications between different cloud providers or between on-premises and cloud environments. Docker Swarm extends these benefits by providing native clustering and orchestration capabilities which are essential in a cloud environment. It helps in maintaining high availability and failover capabilities, making it a good choice for production environments. Modern cloud solutions often require running and managing many containers across multiple machines. Docker Swarm provides the tools and platform to maintain, scale, and coordinate these containers, simplifying complex tasks. In conclusion, Docker and Docker Swarm provide a powerful platform for developing, shipping, and running applications. By containerizing applications and services, teams can become more agile, test more efficiently, and deploy faster with assured consistency. Docker is transforming the way teams think about applications and is a key component of many modern cloud strategies. Read More Docker Docker Get Docker Docker Get Started: Overview Docker (software) on Wikiwand Why Docker? What Does Docker Do and When Should You Use It? Docker Swarm Docker Swarm Deploy What Is Docker Swarm Mode and When Should You Use It? Docker Swarm Services Docker Swarm Tutorial Docker Swarm Command Line Reference Docker Swarm Docker Compose Using Docker Compose Why You Should Use Docker Compose Docker Compose Docker Compose on GitHub What Is Docker Compose and How Do You Use It?","title":"Docker"},{"location":"tools/docker/#docker-and-docker-swarm","text":"","title":"Docker and Docker Swarm"},{"location":"tools/docker/#introduction-to-docker","text":"Docker is an open-source platform designed to automate the deployment, scaling, and operation of applications. It uses containerization technology to wrap software and its dependencies into a standardized unit for software development. Docker containers, which are lightweight and standalone, can run on any machine that installs Docker, regardless of the operating environment.","title":"Introduction to Docker"},{"location":"tools/docker/#why-use-docker","text":"Docker brings several benefits to the software development process: Portability : Since Docker containers encapsulate everything an application needs to run (including the operating system, application code, runtime, system tools, and libraries), they ensure consistency across multiple development and staging environments. Efficiency : Docker containers are lightweight because they use shared operating systems. They take up less space than VMs (virtual machines), and start up and replicate quickly. Version Control and Component Reuse : Docker has built-in version control capabilities that make it easy to track changes, roll back, and ensure consistency. Sharing : Docker has a public registry (Docker Hub) where you can share your containers with others, which is particularly useful for publishing your work and collaborating with others.","title":"Why Use Docker?"},{"location":"tools/docker/#docker-compose","text":"Docker Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application's services, and then with a single command, you can create and start all the services from your configuration. Using Docker Compose can simplify the process of managing multi-container Docker applications. It allows developers to define an application's infrastructure and dependencies in a single file, then spin up the application with a single command.","title":"Docker Compose"},{"location":"tools/docker/#introduction-to-docker-swarm","text":"Docker Swarm, often just referred to as Swarm, is Docker's native clustering and scheduling tool. A swarm is a group of machines that are running Docker and joined into a cluster. After that has been established, you continue to run the Docker commands you're used to, but now they are executed on a cluster by a swarm manager. Swarm uses the standard Docker API, so any tool that already communicates with a Docker daemon can use Swarm to transparently scale to multiple hosts.","title":"Introduction to Docker Swarm"},{"location":"tools/docker/#why-use-docker-swarm","text":"Docker Swarm provides several advantages: Distributed Design : Instead of handling everything on one single Docker host, you can handle it across multiple Docker hosts. Scalability : Docker Swarm allows you to increase the number of container instances as demand increases. High Availability : Docker Swarm provides high availability with features like service replication and service health checking. Security : Docker Swarm uses TLS for authentication, role-based access control, and cryptographic node identity, making it a secure choice for deployment.","title":"Why Use Docker Swarm?"},{"location":"tools/docker/#docker-in-modern-cloud-solutions","text":"Docker is increasingly used in modern cloud solutions for several reasons: Microservices Architecture : Docker is ideal for microservices architecture because it allows each service to run in its own container, making it easy to scale and update services independently. Continuous Integration/Continuous Deployment (CI/CD) : Docker can integrate with popular CI/CD tools like Jenkins, allowing for seamless, efficient, and consistent delivery pipelines. DevOps Practices : Docker aligns with DevOps practices by enabling developers to work in standardized environments using local containers which provide your applications and services. This greatly reduces the \"it works on my machine\" problem. Cloud Portability : Applications packaged as Docker containers can run on any machine that has Docker installed, regardless of the underlying infrastructure. This makes it easy to move applications between different cloud providers or between on-premises and cloud environments. Docker Swarm extends these benefits by providing native clustering and orchestration capabilities which are essential in a cloud environment. It helps in maintaining high availability and failover capabilities, making it a good choice for production environments. Modern cloud solutions often require running and managing many containers across multiple machines. Docker Swarm provides the tools and platform to maintain, scale, and coordinate these containers, simplifying complex tasks. In conclusion, Docker and Docker Swarm provide a powerful platform for developing, shipping, and running applications. By containerizing applications and services, teams can become more agile, test more efficiently, and deploy faster with assured consistency. Docker is transforming the way teams think about applications and is a key component of many modern cloud strategies.","title":"Docker in Modern Cloud Solutions"},{"location":"tools/docker/#read-more","text":"","title":"Read More"},{"location":"tools/docker/#docker","text":"Docker Get Docker Docker Get Started: Overview Docker (software) on Wikiwand Why Docker? What Does Docker Do and When Should You Use It?","title":"Docker"},{"location":"tools/docker/#docker-swarm","text":"Docker Swarm Deploy What Is Docker Swarm Mode and When Should You Use It? Docker Swarm Services Docker Swarm Tutorial Docker Swarm Command Line Reference Docker Swarm","title":"Docker Swarm"},{"location":"tools/docker/#docker-compose_1","text":"Using Docker Compose Why You Should Use Docker Compose Docker Compose Docker Compose on GitHub What Is Docker Compose and How Do You Use It?","title":"Docker Compose"},{"location":"tools/mongodb/","text":"MongoDB: The Backbone of Our Data Management Systems Introduction At our company, we understand the importance of robust, scalable, and flexible data management. We serve a diverse range of clients and users, each with unique needs and expectations. To effectively meet and exceed these expectations, we have chosen MongoDB as the cornerstone of our data management systems. Why MongoDB? Dynamic Schema We operate in an ever-changing environment where the ability to adapt quickly is key. MongoDB's dynamic schema allows us to evolve our applications rapidly. We can modify our data structure as our application evolves without the need for expensive migrations that relational databases often require. Whether we are adding new features or modifying existing ones, MongoDB's flexibility allows us to iterate quickly and adapt to our users' needs. Scalability As we grow and serve more users, we need a database that scales with us. MongoDB's horizontal scaling capabilities, achieved through sharding, enable us to distribute our data across multiple machines. This ensures that as our user base grows, our applications continue to deliver high performance, maintaining a consistent user experience. High Availability Our users expect our systems to be available whenever they need them. MongoDB's replica set feature provides high availability of our data. By maintaining the same data set across different servers, MongoDB allows us to provide uninterrupted service even in the event of a system failure, ensuring that our applications remain reliable and robust. Performance MongoDB's performance is another crucial factor in our choice. Its BSON (Binary JSON) document format and indexing capabilities allow for fast query processing. This enables us to provide our users with speedy responses and a smooth user experience, even when dealing with large volumes of data. Versatility MongoDB is a highly versatile database that can handle a wide variety of data types. This is crucial for us as our applications deal with a range of different data structures. Whether we're dealing with text, geospatial data, or time-series data, MongoDB provides us with the tools to manage and analyze this data effectively. Conclusion In summary, MongoDB's flexibility, scalability, and high performance make it an ideal choice for our data management needs. It provides us with the tools to adapt quickly, scale efficiently, and deliver reliable, high-performance applications. This allows us to focus on what we do best: providing excellent service to our users. By leveraging MongoDB's capabilities, we are better equipped to handle the complexities of modern data management, deliver high-quality services, and drive our business forward. We believe that MongoDB is not just a database; it's a key component of our strategy for delivering superior user experiences. Read More What is MongoDB? Why use MongoDB? MongoDB Tutorial: What is MongoDB? MongoDB Basics: Clusters MongoDB Manual: Replication","title":"MongoDB"},{"location":"tools/mongodb/#mongodb-the-backbone-of-our-data-management-systems","text":"","title":"MongoDB: The Backbone of Our Data Management Systems"},{"location":"tools/mongodb/#introduction","text":"At our company, we understand the importance of robust, scalable, and flexible data management. We serve a diverse range of clients and users, each with unique needs and expectations. To effectively meet and exceed these expectations, we have chosen MongoDB as the cornerstone of our data management systems.","title":"Introduction"},{"location":"tools/mongodb/#why-mongodb","text":"","title":"Why MongoDB?"},{"location":"tools/mongodb/#dynamic-schema","text":"We operate in an ever-changing environment where the ability to adapt quickly is key. MongoDB's dynamic schema allows us to evolve our applications rapidly. We can modify our data structure as our application evolves without the need for expensive migrations that relational databases often require. Whether we are adding new features or modifying existing ones, MongoDB's flexibility allows us to iterate quickly and adapt to our users' needs.","title":"Dynamic Schema"},{"location":"tools/mongodb/#scalability","text":"As we grow and serve more users, we need a database that scales with us. MongoDB's horizontal scaling capabilities, achieved through sharding, enable us to distribute our data across multiple machines. This ensures that as our user base grows, our applications continue to deliver high performance, maintaining a consistent user experience.","title":"Scalability"},{"location":"tools/mongodb/#high-availability","text":"Our users expect our systems to be available whenever they need them. MongoDB's replica set feature provides high availability of our data. By maintaining the same data set across different servers, MongoDB allows us to provide uninterrupted service even in the event of a system failure, ensuring that our applications remain reliable and robust.","title":"High Availability"},{"location":"tools/mongodb/#performance","text":"MongoDB's performance is another crucial factor in our choice. Its BSON (Binary JSON) document format and indexing capabilities allow for fast query processing. This enables us to provide our users with speedy responses and a smooth user experience, even when dealing with large volumes of data.","title":"Performance"},{"location":"tools/mongodb/#versatility","text":"MongoDB is a highly versatile database that can handle a wide variety of data types. This is crucial for us as our applications deal with a range of different data structures. Whether we're dealing with text, geospatial data, or time-series data, MongoDB provides us with the tools to manage and analyze this data effectively.","title":"Versatility"},{"location":"tools/mongodb/#conclusion","text":"In summary, MongoDB's flexibility, scalability, and high performance make it an ideal choice for our data management needs. It provides us with the tools to adapt quickly, scale efficiently, and deliver reliable, high-performance applications. This allows us to focus on what we do best: providing excellent service to our users. By leveraging MongoDB's capabilities, we are better equipped to handle the complexities of modern data management, deliver high-quality services, and drive our business forward. We believe that MongoDB is not just a database; it's a key component of our strategy for delivering superior user experiences.","title":"Conclusion"},{"location":"tools/mongodb/#read-more","text":"What is MongoDB? Why use MongoDB? MongoDB Tutorial: What is MongoDB? MongoDB Basics: Clusters MongoDB Manual: Replication","title":"Read More"},{"location":"tools/security/","text":"Security Introduction This documentation provides a comprehensive overview of our company's server architecture, utilizing WireGuard for server communication, Docker Swarm for service orchestration, and Transport Layer Security (TLS) for secure external connections. This infrastructure incorporates state-of-the-art technologies to ensure high availability, security, and scalability. Server Architecture Our architecture consists of three servers interconnected via a VPN, establishing a robust platform for running distributed services. This setup facilitates high resilience, distributing the workload across multiple nodes, allowing the system to remain functional even if one of the servers fails. The servers communicate via WireGuard, a secure, modern, and efficient VPN protocol. Docker Swarm manages the services running on these servers, ensuring they function efficiently and reliably. External connections to these services are secured using TLS, guaranteeing that all transmitted data remains confidential and secure. WireGuard WireGuard is an open-source VPN designed as a general-purpose VPN, suitable for various circumstances, from embedded interfaces to supercomputers. It uses state-of-the-art cryptography like the Noise protocol framework, Curve25519, ChaCha20, Poly1305, BLAKE2, SipHash24, HKDF, ensuring top-level security. Noteworthy security features of WireGuard include: Cryptographically Sound: WireGuard leverages cutting-edge cryptographic protocols to ensure secure, fast, and reliable connections. Shorter Keys: WireGuard employs shorter keys compared to other VPN protocols, leading to reduced complexity and faster key generation. Perfect Forward Secrecy: WireGuard supports Perfect Forward Secrecy (PFS), securing past communication even if a private key is compromised because it uses ephemeral keys for each connection. Denial of Service (DoS) Resistance: WireGuard's design minimizes data exchanged during connection establishment, reducing the potential for DoS attacks. IP Address Hiding: By default, WireGuard does not include the originating IP address in the data packets, hiding the traffic's source. Docker Swarm Docker Swarm, a container orchestration tool by Docker, Inc., allows us to manage a cluster of servers as a single virtual server. It enables easy deployment, scalability, and high availability of applications. Here are key points about our Docker Swarm setup: Services and Stacks: We organize related services into stacks, each with its own docker-compose.yml file, facilitating easy deployment and management of groups of related services. Load Balancing: Docker Swarm natively supports load balancing of requests between different instances of a service. Service Discovery: Docker's internal DNS server allows services within the Swarm to communicate with each other by name, eliminating the need to hard-code IP addresses. Scaling: We can easily scale up a service by increasing the number of instances (replicas). Docker Swarm will distribute the new instances across the Swarm nodes. Rolling Updates and Rollbacks: Docker Swarm supports rolling updates, allowing us to deploy a new version of a service without downtime. If something goes wrong, we can roll back to the previous version. Docker Swarm also offers various security features: TLS for Nodes: Docker Swarm uses mutual TLS for all node communication, ensuring all communication between nodes is encrypted and authenticated. Security Scanning and Signed Images: Docker Security Scanning and Docker Content Trust identify and mitigate vulnerabilities in the application layers of our Docker images, and prevent tampered images from being deployed. Service Isolation: Docker Swarm enforces network isolation, so each service deployed on the swarm can only communicate with other services via explicitly defined network paths. Secret Management: Docker Swarm provides secure storage of secrets that can be used by services. Secrets are encrypted during transit and at rest, and only shared with services that specifically request them. Docker Swarm's orchestration capabilities ensure high availability, which is a crucial aspect of our overall security strategy: Rolling Updates: Docker Swarm performs rolling updates to services, where new versions of services are gradually rolled out. This not only ensures continuous availability of services, but also means that security updates can be applied without any downtime. If an update fails, Docker Swarm automatically rolls back, reducing the risk of exposure to vulnerabilities. Service Rescheduling: In the event of a node becoming unavailable, Docker Swarm automatically reschedules the services running on that node to other available nodes. This functionality ensures continuous service availability, reducing the potential for attackers to exploit periods of downtime. Load Balancing: Docker Swarm has built-in load balancing that distributes service requests evenly among all instances. This not only improves overall service reliability but also aids in mitigating Distributed Denial of Service (DDoS) attacks by distributing traffic. Scaling: Docker Swarm allows for horizontal scaling, where additional instances of a service can be quickly started to handle increased load. This means that in the event of a sudden traffic surge, perhaps due to a DDoS attack, new instances can be deployed rapidly to maintain service availability. These new instances are automatically balanced across the nodes in the swarm, helping to manage the load and maintain uptime. Transport Layer Security (TLS) Transport Layer Security (TLS) secures connections over a network by encrypting the data sent between an app and its server. We use Let's Encrypt to generate free TLS certificates for each of our services, which are automatically renewed before expiry, ensuring our connections remain secure always. Here are a few key points about our TLS setup: Automated Certificate Management: We use the Certbot tool to automatically issue and renew Let's Encrypt certificates. Strict Transport Security: We use the HTTP Strict Transport Security (HSTS) policy to instruct browsers to always use HTTPS, eliminating the risk of unsecured connections. Forward Secrecy: Our TLS configuration supports forward secrecy, meaning that if a session key is compromised, it can't be used to decrypt past or future sessions. Conclusion Our company's server architecture is designed to provide a secure, robust, and scalable platform for running our services. With WireGuard, Docker Swarm, and TLS at the heart of our infrastructure, we can maintain high availability and security, ensuring our system is resilient to failures, prepared for scaling as needed, and robust against potential security threats. Should you have any questions or need further clarification on any part of the system, please don't hesitate to contact the IT department. We're here to help! Read More Docker Secrets Management Docker Security Best Practices Guide to Docker Swarm Security How Swarm Mode Works - PKI Docker Swarm Mode Overview","title":"Security"},{"location":"tools/security/#security","text":"","title":"Security"},{"location":"tools/security/#introduction","text":"This documentation provides a comprehensive overview of our company's server architecture, utilizing WireGuard for server communication, Docker Swarm for service orchestration, and Transport Layer Security (TLS) for secure external connections. This infrastructure incorporates state-of-the-art technologies to ensure high availability, security, and scalability.","title":"Introduction"},{"location":"tools/security/#server-architecture","text":"Our architecture consists of three servers interconnected via a VPN, establishing a robust platform for running distributed services. This setup facilitates high resilience, distributing the workload across multiple nodes, allowing the system to remain functional even if one of the servers fails. The servers communicate via WireGuard, a secure, modern, and efficient VPN protocol. Docker Swarm manages the services running on these servers, ensuring they function efficiently and reliably. External connections to these services are secured using TLS, guaranteeing that all transmitted data remains confidential and secure.","title":"Server Architecture"},{"location":"tools/security/#wireguard","text":"WireGuard is an open-source VPN designed as a general-purpose VPN, suitable for various circumstances, from embedded interfaces to supercomputers. It uses state-of-the-art cryptography like the Noise protocol framework, Curve25519, ChaCha20, Poly1305, BLAKE2, SipHash24, HKDF, ensuring top-level security. Noteworthy security features of WireGuard include: Cryptographically Sound: WireGuard leverages cutting-edge cryptographic protocols to ensure secure, fast, and reliable connections. Shorter Keys: WireGuard employs shorter keys compared to other VPN protocols, leading to reduced complexity and faster key generation. Perfect Forward Secrecy: WireGuard supports Perfect Forward Secrecy (PFS), securing past communication even if a private key is compromised because it uses ephemeral keys for each connection. Denial of Service (DoS) Resistance: WireGuard's design minimizes data exchanged during connection establishment, reducing the potential for DoS attacks. IP Address Hiding: By default, WireGuard does not include the originating IP address in the data packets, hiding the traffic's source.","title":"WireGuard"},{"location":"tools/security/#docker-swarm","text":"Docker Swarm, a container orchestration tool by Docker, Inc., allows us to manage a cluster of servers as a single virtual server. It enables easy deployment, scalability, and high availability of applications. Here are key points about our Docker Swarm setup: Services and Stacks: We organize related services into stacks, each with its own docker-compose.yml file, facilitating easy deployment and management of groups of related services. Load Balancing: Docker Swarm natively supports load balancing of requests between different instances of a service. Service Discovery: Docker's internal DNS server allows services within the Swarm to communicate with each other by name, eliminating the need to hard-code IP addresses. Scaling: We can easily scale up a service by increasing the number of instances (replicas). Docker Swarm will distribute the new instances across the Swarm nodes. Rolling Updates and Rollbacks: Docker Swarm supports rolling updates, allowing us to deploy a new version of a service without downtime. If something goes wrong, we can roll back to the previous version. Docker Swarm also offers various security features: TLS for Nodes: Docker Swarm uses mutual TLS for all node communication, ensuring all communication between nodes is encrypted and authenticated. Security Scanning and Signed Images: Docker Security Scanning and Docker Content Trust identify and mitigate vulnerabilities in the application layers of our Docker images, and prevent tampered images from being deployed. Service Isolation: Docker Swarm enforces network isolation, so each service deployed on the swarm can only communicate with other services via explicitly defined network paths. Secret Management: Docker Swarm provides secure storage of secrets that can be used by services. Secrets are encrypted during transit and at rest, and only shared with services that specifically request them. Docker Swarm's orchestration capabilities ensure high availability, which is a crucial aspect of our overall security strategy: Rolling Updates: Docker Swarm performs rolling updates to services, where new versions of services are gradually rolled out. This not only ensures continuous availability of services, but also means that security updates can be applied without any downtime. If an update fails, Docker Swarm automatically rolls back, reducing the risk of exposure to vulnerabilities. Service Rescheduling: In the event of a node becoming unavailable, Docker Swarm automatically reschedules the services running on that node to other available nodes. This functionality ensures continuous service availability, reducing the potential for attackers to exploit periods of downtime. Load Balancing: Docker Swarm has built-in load balancing that distributes service requests evenly among all instances. This not only improves overall service reliability but also aids in mitigating Distributed Denial of Service (DDoS) attacks by distributing traffic. Scaling: Docker Swarm allows for horizontal scaling, where additional instances of a service can be quickly started to handle increased load. This means that in the event of a sudden traffic surge, perhaps due to a DDoS attack, new instances can be deployed rapidly to maintain service availability. These new instances are automatically balanced across the nodes in the swarm, helping to manage the load and maintain uptime.","title":"Docker Swarm"},{"location":"tools/security/#transport-layer-security-tls","text":"Transport Layer Security (TLS) secures connections over a network by encrypting the data sent between an app and its server. We use Let's Encrypt to generate free TLS certificates for each of our services, which are automatically renewed before expiry, ensuring our connections remain secure always. Here are a few key points about our TLS setup: Automated Certificate Management: We use the Certbot tool to automatically issue and renew Let's Encrypt certificates. Strict Transport Security: We use the HTTP Strict Transport Security (HSTS) policy to instruct browsers to always use HTTPS, eliminating the risk of unsecured connections. Forward Secrecy: Our TLS configuration supports forward secrecy, meaning that if a session key is compromised, it can't be used to decrypt past or future sessions.","title":"Transport Layer Security (TLS)"},{"location":"tools/security/#conclusion","text":"Our company's server architecture is designed to provide a secure, robust, and scalable platform for running our services. With WireGuard, Docker Swarm, and TLS at the heart of our infrastructure, we can maintain high availability and security, ensuring our system is resilient to failures, prepared for scaling as needed, and robust against potential security threats. Should you have any questions or need further clarification on any part of the system, please don't hesitate to contact the IT department. We're here to help!","title":"Conclusion"},{"location":"tools/security/#read-more","text":"Docker Secrets Management Docker Security Best Practices Guide to Docker Swarm Security How Swarm Mode Works - PKI Docker Swarm Mode Overview","title":"Read More"},{"location":"tools/vue/","text":"Vue.js - The Progressive JavaScript Framework for Modern Cloud Solutions Introduction Vue.js, often referred to simply as Vue, is a modern progressive JavaScript framework used for building user interfaces. Unlike other monolithic frameworks, Vue is designed from the ground up to be incrementally adoptable. This means that the core library focuses solely on the view layer, making it easy to integrate with other libraries or existing projects. What is Vue.js? Vue.js is a popular JavaScript framework for building user interfaces. Vue, which means 'view' in French, is fitting for a framework that focuses on the view layer. Vue.js adopts the single-page application (SPA) architecture, allowing developers to create powerful, fast applications that deliver an outstanding user experience. Vue.js History Vue.js was first released in 2014 by Evan You, a former Google engineer. He had previously worked on AngularJS projects at Google and decided to extract what he saw as the best parts of Angular, building a lightweight and flexible framework that avoids unnecessary complexity. This project ended up as Vue.js. Why Vue.js in Modern Cloud Solutions? There are several reasons why Vue.js is an excellent choice for modern cloud solutions: 1. Easy Integration Vue.js is built to be incrementally adoptable, making it very easy to integrate with existing projects. This is particularly useful in microservices-based cloud architectures, where each service can potentially be a separate application. 2. Performance Vue.js is lightweight and has a small footprint, both of which contribute to high performance. This is particularly important in cloud environments, where resources are often metered. 3. Scalability Vue.js is designed with scalability in mind. It's easy to scale Vue.js applications horizontally in cloud environments, due to the framework's design and the SPA architecture. 4. Flexibility Vue.js supports a range of development paradigms, including declarative programming, imperative programming, and object-oriented programming. This flexibility makes Vue.js suitable for a wide range of applications and use cases. 5. Ecosystem The Vue.js ecosystem includes tools like Vuex for state management and Vue Router for routing, and supports a wide range of plugins and add-ons. This makes it easy to build complex applications with Vue.js. Conclusion In summary, Vue.js is a versatile, performant, and easy-to-use JavaScript framework that's well-suited to building modern cloud applications. With its flexibility, scalability, and powerful ecosystem, Vue.js is a strong choice for any cloud solution. Read More Vue.js for Large Scale Applications Why Vue.js is Gaining Popularity Dockerize Vue.js App (Vue CLI 2) Vue.js Introduction Guide Why Use Vue.js?","title":"Vue"},{"location":"tools/vue/#vuejs-the-progressive-javascript-framework-for-modern-cloud-solutions","text":"","title":"Vue.js - The Progressive JavaScript Framework for Modern Cloud Solutions"},{"location":"tools/vue/#introduction","text":"Vue.js, often referred to simply as Vue, is a modern progressive JavaScript framework used for building user interfaces. Unlike other monolithic frameworks, Vue is designed from the ground up to be incrementally adoptable. This means that the core library focuses solely on the view layer, making it easy to integrate with other libraries or existing projects.","title":"Introduction"},{"location":"tools/vue/#what-is-vuejs","text":"Vue.js is a popular JavaScript framework for building user interfaces. Vue, which means 'view' in French, is fitting for a framework that focuses on the view layer. Vue.js adopts the single-page application (SPA) architecture, allowing developers to create powerful, fast applications that deliver an outstanding user experience.","title":"What is Vue.js?"},{"location":"tools/vue/#vuejs-history","text":"Vue.js was first released in 2014 by Evan You, a former Google engineer. He had previously worked on AngularJS projects at Google and decided to extract what he saw as the best parts of Angular, building a lightweight and flexible framework that avoids unnecessary complexity. This project ended up as Vue.js.","title":"Vue.js History"},{"location":"tools/vue/#why-vuejs-in-modern-cloud-solutions","text":"There are several reasons why Vue.js is an excellent choice for modern cloud solutions:","title":"Why Vue.js in Modern Cloud Solutions?"},{"location":"tools/vue/#1-easy-integration","text":"Vue.js is built to be incrementally adoptable, making it very easy to integrate with existing projects. This is particularly useful in microservices-based cloud architectures, where each service can potentially be a separate application.","title":"1. Easy Integration"},{"location":"tools/vue/#2-performance","text":"Vue.js is lightweight and has a small footprint, both of which contribute to high performance. This is particularly important in cloud environments, where resources are often metered.","title":"2. Performance"},{"location":"tools/vue/#3-scalability","text":"Vue.js is designed with scalability in mind. It's easy to scale Vue.js applications horizontally in cloud environments, due to the framework's design and the SPA architecture.","title":"3. Scalability"},{"location":"tools/vue/#4-flexibility","text":"Vue.js supports a range of development paradigms, including declarative programming, imperative programming, and object-oriented programming. This flexibility makes Vue.js suitable for a wide range of applications and use cases.","title":"4. Flexibility"},{"location":"tools/vue/#5-ecosystem","text":"The Vue.js ecosystem includes tools like Vuex for state management and Vue Router for routing, and supports a wide range of plugins and add-ons. This makes it easy to build complex applications with Vue.js.","title":"5. Ecosystem"},{"location":"tools/vue/#conclusion","text":"In summary, Vue.js is a versatile, performant, and easy-to-use JavaScript framework that's well-suited to building modern cloud applications. With its flexibility, scalability, and powerful ecosystem, Vue.js is a strong choice for any cloud solution.","title":"Conclusion"},{"location":"tools/vue/#read-more","text":"Vue.js for Large Scale Applications Why Vue.js is Gaining Popularity Dockerize Vue.js App (Vue CLI 2) Vue.js Introduction Guide Why Use Vue.js?","title":"Read More"}]}